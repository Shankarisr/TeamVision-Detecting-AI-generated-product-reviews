{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26ee2821e3d542229dd1638d12828b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6cea84a42084d199ffc8d65acda3fd4",
              "IPY_MODEL_ea2b131b6c504c3ba731fb02fa21c75b",
              "IPY_MODEL_49c2fdb9f08e42229768ed7969a3295a"
            ],
            "layout": "IPY_MODEL_8a365541f1bd4efa9ce5e5ad10035bb6"
          }
        },
        "a6cea84a42084d199ffc8d65acda3fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eee0e44e26b4b2b8c3594e36d13694e",
            "placeholder": "​",
            "style": "IPY_MODEL_b7af2807b8624f47846677d231e30d87",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ea2b131b6c504c3ba731fb02fa21c75b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09aad75c74f148f98502bc7fcca2981f",
            "max": 49,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b701fac79d54135b170c2360df74202",
            "value": 49
          }
        },
        "49c2fdb9f08e42229768ed7969a3295a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc28297c5e5d46f5aa36e443556616de",
            "placeholder": "​",
            "style": "IPY_MODEL_4815cd41cebb4616b5b35fa1d4d3f175",
            "value": " 49.0/49.0 [00:00&lt;00:00, 2.34kB/s]"
          }
        },
        "8a365541f1bd4efa9ce5e5ad10035bb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eee0e44e26b4b2b8c3594e36d13694e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7af2807b8624f47846677d231e30d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09aad75c74f148f98502bc7fcca2981f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b701fac79d54135b170c2360df74202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc28297c5e5d46f5aa36e443556616de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4815cd41cebb4616b5b35fa1d4d3f175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32ebfb69742b42ab9b66e2c16894f8c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc46f55fbb1a42a5aeb70bda3196ca94",
              "IPY_MODEL_d359bb3b8bf14411af51090b81626b6a",
              "IPY_MODEL_d2ba44c5e43443aca094e16b36227b91"
            ],
            "layout": "IPY_MODEL_b9c025aa13214925a494983492c5bfe3"
          }
        },
        "bc46f55fbb1a42a5aeb70bda3196ca94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eece71bfa59d4b5e83d450c213a28f8e",
            "placeholder": "​",
            "style": "IPY_MODEL_15c3cb0da9e94bcf8a39db551dbbed92",
            "value": "config.json: 100%"
          }
        },
        "d359bb3b8bf14411af51090b81626b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_318e5b691d7840a98c48572efc21a45c",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3675badaaa4e4e86a7f3f34992909f16",
            "value": 625
          }
        },
        "d2ba44c5e43443aca094e16b36227b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3c71654b22740ba8401433838c45083",
            "placeholder": "​",
            "style": "IPY_MODEL_7b2492e19e5440e88dd09478895f94e8",
            "value": " 625/625 [00:00&lt;00:00, 36.1kB/s]"
          }
        },
        "b9c025aa13214925a494983492c5bfe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eece71bfa59d4b5e83d450c213a28f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c3cb0da9e94bcf8a39db551dbbed92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "318e5b691d7840a98c48572efc21a45c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3675badaaa4e4e86a7f3f34992909f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3c71654b22740ba8401433838c45083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b2492e19e5440e88dd09478895f94e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "265e64a983064e0d94e44bbe4bbecadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90aba49bfb5e4ff3bb11bd627f89b20d",
              "IPY_MODEL_f5042a6ca5514830bdb49139c61b4792",
              "IPY_MODEL_a081f9217cd04275bafa10100979776d"
            ],
            "layout": "IPY_MODEL_777c50fea38d469f811f334adcccc7cc"
          }
        },
        "90aba49bfb5e4ff3bb11bd627f89b20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5fe60f3922943e9a8d6760c003d8e58",
            "placeholder": "​",
            "style": "IPY_MODEL_6d2dba46b4024fb6a67ab3f4e5569136",
            "value": "vocab.txt: 100%"
          }
        },
        "f5042a6ca5514830bdb49139c61b4792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_985fbed3bf6249d7b4c8f96b75f4f28a",
            "max": 995526,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a303cc59be334d1ca2c31bbe5b76994e",
            "value": 995526
          }
        },
        "a081f9217cd04275bafa10100979776d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe2748fe887e48ae994db3b2fcd91fc1",
            "placeholder": "​",
            "style": "IPY_MODEL_88a3745bfce64bf19e9a0b7b223835e5",
            "value": " 996k/996k [00:00&lt;00:00, 13.2MB/s]"
          }
        },
        "777c50fea38d469f811f334adcccc7cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5fe60f3922943e9a8d6760c003d8e58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d2dba46b4024fb6a67ab3f4e5569136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "985fbed3bf6249d7b4c8f96b75f4f28a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a303cc59be334d1ca2c31bbe5b76994e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe2748fe887e48ae994db3b2fcd91fc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88a3745bfce64bf19e9a0b7b223835e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e57d71c134a4403f8fb2897c5369a471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e22a53b6082547b38b4a8134251230cc",
              "IPY_MODEL_f3198999fd7245b08a0c1e2b92b2c936",
              "IPY_MODEL_ba3281d8c0f9453cadd3abe2487cbf9d"
            ],
            "layout": "IPY_MODEL_f9bc2e0027ec47848e689adcd8b18afa"
          }
        },
        "e22a53b6082547b38b4a8134251230cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2ac48e8e10a4a67b1328c598ce75dae",
            "placeholder": "​",
            "style": "IPY_MODEL_455b89ae304b409e8969e909c90d880f",
            "value": "tokenizer.json: 100%"
          }
        },
        "f3198999fd7245b08a0c1e2b92b2c936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_681ec8af1267439aaf19bb5cccdaf936",
            "max": 1961828,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4477bc6f8514a00b6748578574f7d39",
            "value": 1961828
          }
        },
        "ba3281d8c0f9453cadd3abe2487cbf9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cf5031165154362b0603d8faf3f3cce",
            "placeholder": "​",
            "style": "IPY_MODEL_01d61b487c8a4ef8a333cef99510d7a2",
            "value": " 1.96M/1.96M [00:00&lt;00:00, 22.5MB/s]"
          }
        },
        "f9bc2e0027ec47848e689adcd8b18afa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ac48e8e10a4a67b1328c598ce75dae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "455b89ae304b409e8969e909c90d880f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "681ec8af1267439aaf19bb5cccdaf936": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4477bc6f8514a00b6748578574f7d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cf5031165154362b0603d8faf3f3cce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01d61b487c8a4ef8a333cef99510d7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aca58216961d4689adca2144760294e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f48371f164424dc795c48a4371795938",
              "IPY_MODEL_8ed377205dcb423c8b1faa86b0cb9a9c",
              "IPY_MODEL_8947201c84a64d989a3c53cd1a245128"
            ],
            "layout": "IPY_MODEL_8562cb1aa5564549b0cbc14939eeac01"
          }
        },
        "f48371f164424dc795c48a4371795938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9b65b3261624a0b9cf7216791080a61",
            "placeholder": "​",
            "style": "IPY_MODEL_2fd62e63e71b42e7ba7c2230be3ea79b",
            "value": "model.safetensors: 100%"
          }
        },
        "8ed377205dcb423c8b1faa86b0cb9a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b898dbaccec4dfcb1e7072b8060e18c",
            "max": 714290682,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b68cd27bff934606a920c7bf3b154e46",
            "value": 714290682
          }
        },
        "8947201c84a64d989a3c53cd1a245128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b2d6eeafdcc4ea099b74c5b875e612d",
            "placeholder": "​",
            "style": "IPY_MODEL_bd01fc07699a416a92beb8111fc31441",
            "value": " 714M/714M [00:06&lt;00:00, 152MB/s]"
          }
        },
        "8562cb1aa5564549b0cbc14939eeac01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9b65b3261624a0b9cf7216791080a61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd62e63e71b42e7ba7c2230be3ea79b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b898dbaccec4dfcb1e7072b8060e18c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68cd27bff934606a920c7bf3b154e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b2d6eeafdcc4ea099b74c5b875e612d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd01fc07699a416a92beb8111fc31441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive"
      ],
      "metadata": {
        "id": "gnh70i-OMdS7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iIMe7tDwK4h",
        "outputId": "bf897aed-16f1-48e3-e0f6-c052f91a1d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "#file path\n",
        "tamil_dataset = '/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv'\n",
        "\n",
        "\n",
        "# Step 3: Load the Dataset\n",
        "# Assuming it's a CSV file\n",
        "dataset = pd.read_csv(tamil_dataset)\n",
        "\n",
        "\n",
        "# Step 4: Display the Dataset\n",
        "# Show the first few rows\n",
        "print(dataset.head())\n",
        "\n",
        "print()\n",
        "# Show information about the dataset\n",
        "print(dataset.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aee7shZ8x5a_",
        "outputId": "807e4972-18b4-4aa4-a6a1-8b1f759005a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                ID                                               DATA LABEL\n",
            "0  TAM_HUAI_TR_001  இந்த சோப்பின் மணம் மிகவும் புத்துணர்ச்சியூட்டு...    AI\n",
            "1  TAM_HUAI_TR_002   தோலை நன்கு சுத்தம் செய்ய இது மிகவும் சிறப்பானது.    AI\n",
            "2  TAM_HUAI_TR_003  இதைப் பயன்படுத்திய பிறகு, தோல் மிக மென்மையாக உ...    AI\n",
            "3  TAM_HUAI_TR_004  இந்த சோப்பில் இயற்கையான மூலப்பொருட்கள் பயன்படு...    AI\n",
            "4  TAM_HUAI_TR_005        சிறிது சோப்பு போதும், அதிக நுரை உருவாகிறது.    AI\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 808 entries, 0 to 807\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   ID      808 non-null    object\n",
            " 1   DATA    808 non-null    object\n",
            " 2   LABEL   808 non-null    object\n",
            "dtypes: object(3)\n",
            "memory usage: 19.1+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing and Split Up**"
      ],
      "metadata": {
        "id": "4_l8DdNv-hNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Tamil stop words (You can expand this list as needed)\n",
        "tamil_stopwords = [\n",
        "    'அது', 'இது', 'அவ்', 'அவர்', 'எனது', 'இந்த', 'என்று', 'ஒரு', 'இலை', 'உள்ள', 'இல்', 'எனக்கு', 'என', 'அந்த', 'உங்கள்', 'நான்', 'இங்கு'\n",
        "    # Add more Tamil stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sஅஆஇஈஉஊஎஎஐஒஓக்களளழவமயரலவறனபஙஜஞசடணதபணமோ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in tamil_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the splits for further processing\n",
        "print(\"Data preprocessing and splitting completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr2inHVD8eRH",
        "outputId": "b11d6dc2-939e-45f8-866e-7c43f9e6341d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing and splitting completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes with feature extraction"
      ],
      "metadata": {
        "id": "ey4yr0wREdxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "QfyKynNaF83g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Tamil stop words (expand as needed)\n",
        "tamil_stopwords = [\n",
        "    'அது', 'இது', 'அவ்', 'அவர்', 'எனது', 'இந்த', 'என்று', 'ஒரு', 'இலை', 'உள்ள', 'இல்', 'எனக்கு', 'என', 'அந்த', 'உங்கள்', 'நான்', 'இங்கு'\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    return re.sub(r'[^\\w\\sஅஆஇஈஉஊஎஎஐஒஓக்களளழவமயரலவறனபஙஜஞசடணதபணமோ]', '', str(text_data))\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    return re.split(' ', text_data)\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in tamil_stopwords]\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_text(df):\n",
        "    # Apply preprocessing steps\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "    return df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I31vLhyfF_Rj",
        "outputId": "2a423498-d1b8-4d1a-f847-69f65224e0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of words"
      ],
      "metadata": {
        "id": "0F5XyjiWOFPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv'  # Adjust the path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocess the dataset to add the 'processed_text' column\n",
        "data = preprocess_text(data)\n",
        "\n",
        "# Extract features and labels\n",
        "X = data['processed_text']  # Ensure processed_text column exists\n",
        "y = data['LABEL']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text to Bag of Words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_bow, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred = nb_classifier.predict(X_test_bow)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the model and vectorizer\n",
        "import joblib\n",
        "joblib.dump(nb_classifier, 'naive_bayes_model.pkl')\n",
        "joblib.dump(vectorizer, 'bow_vectorizer.pkl')\n",
        "\n",
        "print(\"Model and vectorizer saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqXw1YsnGBax",
        "outputId": "4899a9af-ab44-4b18-ce43-9c32d3640092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8765432098765432\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.85      0.93      0.89        86\n",
            "       HUMAN       0.91      0.82      0.86        76\n",
            "\n",
            "    accuracy                           0.88       162\n",
            "   macro avg       0.88      0.87      0.88       162\n",
            "weighted avg       0.88      0.88      0.88       162\n",
            "\n",
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF"
      ],
      "metadata": {
        "id": "bl-qILfiGc-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming `processed_data` is already created using your preprocessing code\n",
        "# Extract features and labels\n",
        "X = processed_data['processed_text']  # Preprocessed text column\n",
        "y = processed_data['LABEL']           # Label column\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF representation\n",
        "tfidf_vectorizer = TfidfVectorizer()  # Initialize TF-IDF vectorizer\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)  # Fit and transform training data\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)        # Transform test data\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "nb_classifier = MultinomialNB()  # Initialize Naive Bayes classifier\n",
        "nb_classifier.fit(X_train_tfidf, y_train)  # Train the model\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw_j5zVLGepR",
        "outputId": "945a44b8-2ab5-4107-b48e-1ca2bd49c81a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8703703703703703\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.84      0.94      0.89        86\n",
            "       HUMAN       0.92      0.79      0.85        76\n",
            "\n",
            "    accuracy                           0.87       162\n",
            "   macro avg       0.88      0.87      0.87       162\n",
            "weighted avg       0.88      0.87      0.87       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for TF-IDF with N-grams"
      ],
      "metadata": {
        "id": "Qhlgtcs_GnWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming `processed_data` is already created using your preprocessing code\n",
        "# Extract features and labels\n",
        "X = processed_data['processed_text']  # Preprocessed text column\n",
        "y = processed_data['LABEL']           # Label column\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF representation with N-grams\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # ngram_range=(1, 2) for unigrams and bigrams\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)  # Fit and transform training data\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)        # Transform test data\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "nb_classifier = MultinomialNB()  # Initialize Naive Bayes classifier\n",
        "nb_classifier.fit(X_train_tfidf, y_train)  # Train the model\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFqxWEv2Goss",
        "outputId": "b755bd53-7417-491a-d700-c5af011e9004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8950617283950617\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.85      0.98      0.91        86\n",
            "       HUMAN       0.97      0.80      0.88        76\n",
            "\n",
            "    accuracy                           0.90       162\n",
            "   macro avg       0.91      0.89      0.89       162\n",
            "weighted avg       0.90      0.90      0.89       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Count Vectorization with N-grams"
      ],
      "metadata": {
        "id": "FBSAXawVHAW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming `processed_data` is already created using your preprocessing code\n",
        "# Extract features and labels\n",
        "X = processed_data['processed_text']  # Preprocessed text column\n",
        "y = processed_data['LABEL']           # Label column\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to Count Vectorization with N-grams\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1, 2))  # ngram_range=(1, 2) for unigrams and bigrams\n",
        "X_train_count = count_vectorizer.fit_transform(X_train)  # Fit and transform training data\n",
        "X_test_count = count_vectorizer.transform(X_test)        # Transform test data\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "nb_classifier = MultinomialNB()  # Initialize Naive Bayes classifier\n",
        "nb_classifier.fit(X_train_count, y_train)  # Train the model\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = nb_classifier.predict(X_test_count)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SrYGEMbHBLi",
        "outputId": "0a46374f-2545-4acb-c7ba-6332fef61cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9012345679012346\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.85      0.99      0.91        86\n",
            "       HUMAN       0.98      0.80      0.88        76\n",
            "\n",
            "    accuracy                           0.90       162\n",
            "   macro avg       0.92      0.90      0.90       162\n",
            "weighted avg       0.91      0.90      0.90       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming `processed_data` is already created using your preprocessing code\n",
        "# Extract features and labels\n",
        "X = processed_data['processed_text']  # Preprocessed text column\n",
        "y = processed_data['LABEL']           # Label column\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to Count Vectorization with N-grams\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1, 2))  # ngram_range=(1, 2) for unigrams and bigrams\n",
        "X_train_count = count_vectorizer.fit_transform(X_train)  # Fit and transform training data\n",
        "X_test_count = count_vectorizer.transform(X_test)        # Transform test data\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "nb_classifier = MultinomialNB()  # Initialize Naive Bayes classifier\n",
        "nb_classifier.fit(X_train_count, y_train)  # Train the model\n",
        "\n",
        "# Save the trained CountVectorizer and Naive Bayes model\n",
        "joblib.dump(count_vectorizer, 'count_vectorizer.pkl')  # Save the CountVectorizer\n",
        "joblib.dump(nb_classifier, 'naive_bayes_model.pkl')   # Save the Naive Bayes model\n",
        "\n",
        "print(\"Models saved as 'count_vectorizer.pkl' and 'naive_bayes_model.pkl'.\")\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = nb_classifier.predict(X_test_count)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4lSb71Z9oud",
        "outputId": "9dd4823f-8e50-4a95-ad36-518a4168b88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models saved as 'count_vectorizer.pkl' and 'naive_bayes_model.pkl'.\n",
            "Accuracy: 0.9012345679012346\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.85      0.99      0.91        86\n",
            "       HUMAN       0.98      0.80      0.88        76\n",
            "\n",
            "    accuracy                           0.90       162\n",
            "   macro avg       0.92      0.90      0.90       162\n",
            "weighted avg       0.91      0.90      0.90       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Tamil stop words (You can expand this list as needed)\n",
        "tamil_stopwords = [\n",
        "    'அது', 'இது', 'அவ்', 'அவர்', 'எனது', 'இந்த', 'என்று', 'ஒரு', 'இலை', 'உள்ள', 'இல்', 'எனக்கு', 'என', 'அந்த', 'உங்கள்', 'நான்', 'இங்கு'\n",
        "    # Add more Tamil stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sஅஆஇஈஉஊஎஎஐஒஓக்களளழவமயரலவறனபஙஜஞசடணதபணமோ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in tamil_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# ------------------------------ 1. Naive Bayes with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb_bow = MultinomialNB()\n",
        "nb_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = nb_bow.predict(X_test_bow)\n",
        "print(\"Naive Bayes with Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. Naive Bayes with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb_tfidf = MultinomialNB()\n",
        "nb_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
        "print(\"Naive Bayes with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. Naive Bayes with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb_ngram_tfidf = MultinomialNB()\n",
        "nb_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = nb_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"Naive Bayes with N-gram and TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. Naive Bayes with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb_ngram = MultinomialNB()\n",
        "nb_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = nb_ngram.predict(X_test_ngram)\n",
        "print(\"Naive Bayes with Count Vectorization and N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suYY2uAcRONI",
        "outputId": "b159741e-52a8-4b0a-f142-159cda810244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes with Bag of Words - Accuracy: 0.8765432098765432\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.85      0.93      0.89        86\n",
            "       HUMAN       0.91      0.82      0.86        76\n",
            "\n",
            "    accuracy                           0.88       162\n",
            "   macro avg       0.88      0.87      0.88       162\n",
            "weighted avg       0.88      0.88      0.88       162\n",
            "\n",
            "Naive Bayes with TF-IDF - Accuracy: 0.8703703703703703\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.84      0.94      0.89        86\n",
            "       HUMAN       0.92      0.79      0.85        76\n",
            "\n",
            "    accuracy                           0.87       162\n",
            "   macro avg       0.88      0.87      0.87       162\n",
            "weighted avg       0.88      0.87      0.87       162\n",
            "\n",
            "Naive Bayes with N-gram and TF-IDF - Accuracy: 0.8950617283950617\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.85      0.98      0.91        86\n",
            "       HUMAN       0.97      0.80      0.88        76\n",
            "\n",
            "    accuracy                           0.90       162\n",
            "   macro avg       0.91      0.89      0.89       162\n",
            "weighted avg       0.90      0.90      0.89       162\n",
            "\n",
            "Naive Bayes with Count Vectorization and N-gram - Accuracy: 0.9012345679012346\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.85      0.99      0.91        86\n",
            "       HUMAN       0.98      0.80      0.88        76\n",
            "\n",
            "    accuracy                           0.90       162\n",
            "   macro avg       0.92      0.90      0.90       162\n",
            "weighted avg       0.91      0.90      0.90       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes output download"
      ],
      "metadata": {
        "id": "pKIRDzB0Mym6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib  # For loading saved models\n",
        "\n",
        "# Paths to files\n",
        "test_data_path = \"/content/drive/Shareddrives/AI Text Detecting/tamil_test.xlsx\"  # Replace with your test data file path\n",
        "output_tsv_path = \"new_naive_test_data_with_generated_labels.tsv\"  # Path to save the output TSV file\n",
        "\n",
        "# Load test data\n",
        "test_data = pd.read_excel(test_data_path)  # Ensure your test file has a column with preprocessed text data\n",
        "test_texts = test_data[\"DATA\"]  # Replace \"DATA\" with the actual column name containing the text data\n",
        "\n",
        "# Load the saved CountVectorizer and Naive Bayes model\n",
        "count_vectorizer = joblib.load(\"count_vectorizer.pkl\")  # Load the fitted vectorizer\n",
        "nb_classifier = joblib.load(\"naive_bayes_model.pkl\")  # Load the trained Naive Bayes classifier\n",
        "\n",
        "# Transform the preprocessed test data using the loaded vectorizer\n",
        "X_test_count = count_vectorizer.transform(test_texts)\n",
        "\n",
        "# Predict labels for the test data using the loaded model\n",
        "predicted_labels = nb_classifier.predict(X_test_count)\n",
        "\n",
        "# Check and normalize the predicted labels to match the mapping\n",
        "if isinstance(predicted_labels[0], str):\n",
        "    # Convert string labels to numerical for consistent mapping\n",
        "    label_mapping_reverse = {\"HUMAN\": 0, \"AI\": 1}  # Reverse mapping for string labels\n",
        "    predicted_labels_numerical = [label_mapping_reverse[label] for label in predicted_labels]\n",
        "else:\n",
        "    predicted_labels_numerical = predicted_labels\n",
        "\n",
        "# Map numerical labels to human-readable labels (adjust based on your trained model's label mapping)\n",
        "label_mapping = {0: \"Human\", 1: \"AI\"}  # Replace or adjust this mapping if needed\n",
        "test_data[\"LABEL\"] = [label_mapping[label] for label in predicted_labels_numerical]\n",
        "\n",
        "# Save the test data with generated labels (including ID column) to a TSV file\n",
        "test_data[[\"ID\", \"DATA\", \"LABEL\"]].to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_tsv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNYTSzCd9YgL",
        "outputId": "4709ce7c-fa4c-474f-feb8-7dd9c664fee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to new_naive_test_data_with_generated_labels.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression with Feature extraction"
      ],
      "metadata": {
        "id": "25VugKBQOwEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Tamil stop words (You can expand this list as needed)\n",
        "tamil_stopwords = [\n",
        "    'அது', 'இது', 'அவ்', 'அவர்', 'எனது', 'இந்த', 'என்று', 'ஒரு', 'இலை', 'உள்ள', 'இல்', 'எனக்கு', 'என', 'அந்த', 'உங்கள்', 'நான்', 'இங்கு'\n",
        "    # Add more Tamil stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sஅஆஇஈஉஊஎஎஐஒஓக்களளழவமயரலவறனபஙஜஞசடணதபணமோ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in tamil_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ------------------------------ 1. Logistic Regression with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logreg_bow = LogisticRegression(max_iter=1000)\n",
        "logreg_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = logreg_bow.predict(X_test_bow)\n",
        "print(\"Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. Logistic Regression with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logreg_tfidf = LogisticRegression(max_iter=1000)\n",
        "logreg_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = logreg_tfidf.predict(X_test_tfidf)\n",
        "print(\"TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. Logistic Regression with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logreg_ngram_tfidf = LogisticRegression(max_iter=1000)\n",
        "logreg_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = logreg_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"N-gram with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. Logistic Regression with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logreg_ngram = LogisticRegression(max_iter=1000)\n",
        "logreg_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = logreg_ngram.predict(X_test_ngram)\n",
        "print(\"Count Vectorization with N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz3zj6U2PP5w",
        "outputId": "80e3e79c-a4c6-41d7-b1dc-370f1bfbd6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words - Accuracy: 0.8703703703703703\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.91      0.84      0.87        86\n",
            "       HUMAN       0.83      0.91      0.87        76\n",
            "\n",
            "    accuracy                           0.87       162\n",
            "   macro avg       0.87      0.87      0.87       162\n",
            "weighted avg       0.87      0.87      0.87       162\n",
            "\n",
            "TF-IDF - Accuracy: 0.8703703703703703\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.87      0.90      0.88        86\n",
            "       HUMAN       0.88      0.84      0.86        76\n",
            "\n",
            "    accuracy                           0.87       162\n",
            "   macro avg       0.87      0.87      0.87       162\n",
            "weighted avg       0.87      0.87      0.87       162\n",
            "\n",
            "N-gram with TF-IDF - Accuracy: 0.8765432098765432\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.86      0.92      0.89        86\n",
            "       HUMAN       0.90      0.83      0.86        76\n",
            "\n",
            "    accuracy                           0.88       162\n",
            "   macro avg       0.88      0.87      0.88       162\n",
            "weighted avg       0.88      0.88      0.88       162\n",
            "\n",
            "Count Vectorization with N-gram - Accuracy: 0.8827160493827161\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.91      0.86      0.89        86\n",
            "       HUMAN       0.85      0.91      0.88        76\n",
            "\n",
            "    accuracy                           0.88       162\n",
            "   macro avg       0.88      0.88      0.88       162\n",
            "weighted avg       0.88      0.88      0.88       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "JQdK1l-YPlq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Tamil stop words (You can expand this list as needed)\n",
        "tamil_stopwords = [\n",
        "    'அது', 'இது', 'அவ்', 'அவர்', 'எனது', 'இந்த', 'என்று', 'ஒரு', 'இலை', 'உள்ள', 'இல்', 'எனக்கு', 'என', 'அந்த', 'உங்கள்', 'நான்', 'இங்கு'\n",
        "    # Add more Tamil stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sஅஆஇஈஉஊஎஎஐஒஓக்களளழவமயரலவறனபஙஜஞசடணதபணமோ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in tamil_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# ------------------------------ 1. SVM with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM model\n",
        "svm_bow = SVC(kernel='linear')\n",
        "svm_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = svm_bow.predict(X_test_bow)\n",
        "print(\"SVM with Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. SVM with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM model\n",
        "svm_tfidf = SVC(kernel='linear')\n",
        "svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
        "print(\"SVM with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. SVM with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM model\n",
        "svm_ngram_tfidf = SVC(kernel='linear')\n",
        "svm_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = svm_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"SVM with N-gram and TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. SVM with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM model\n",
        "svm_ngram = SVC(kernel='linear')\n",
        "svm_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = svm_ngram.predict(X_test_ngram)\n",
        "print(\"SVM with Count Vectorization and N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlEB8lQlPrnT",
        "outputId": "2243eb2a-0b37-464a-d6d5-df73e61f8b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM with Bag of Words - Accuracy: 0.8888888888888888\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.93      0.86      0.89        86\n",
            "       HUMAN       0.85      0.92      0.89        76\n",
            "\n",
            "    accuracy                           0.89       162\n",
            "   macro avg       0.89      0.89      0.89       162\n",
            "weighted avg       0.89      0.89      0.89       162\n",
            "\n",
            "SVM with TF-IDF - Accuracy: 0.8950617283950617\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.89      0.92      0.90        86\n",
            "       HUMAN       0.90      0.87      0.89        76\n",
            "\n",
            "    accuracy                           0.90       162\n",
            "   macro avg       0.90      0.89      0.89       162\n",
            "weighted avg       0.90      0.90      0.89       162\n",
            "\n",
            "SVM with N-gram and TF-IDF - Accuracy: 0.8950617283950617\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.88      0.93      0.90        86\n",
            "       HUMAN       0.92      0.86      0.88        76\n",
            "\n",
            "    accuracy                           0.90       162\n",
            "   macro avg       0.90      0.89      0.89       162\n",
            "weighted avg       0.90      0.90      0.89       162\n",
            "\n",
            "SVM with Count Vectorization and N-gram - Accuracy: 0.8765432098765432\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.92      0.84      0.88        86\n",
            "       HUMAN       0.83      0.92      0.88        76\n",
            "\n",
            "    accuracy                           0.88       162\n",
            "   macro avg       0.88      0.88      0.88       162\n",
            "weighted avg       0.88      0.88      0.88       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM model\n",
        "svm_tfidf = SVC(kernel='linear')\n",
        "svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Save the trained model and vectorizer\n",
        "joblib.dump(tfidf_vectorizer, \"tfidf_vectorizer.pkl\")  # Save TF-IDF vectorizer\n",
        "joblib.dump(svm_tfidf, \"svm_tfidf_model.pkl\")         # Save SVM model\n",
        "\n",
        "print(\"TF-IDF vectorizer and SVM model saved successfully.\")\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
        "print(\"SVM with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar8lziRrE9Bl",
        "outputId": "f5fad7c9-51c9-48cc-b2c6-43b67faabc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF vectorizer and SVM model saved successfully.\n",
            "SVM with TF-IDF - Accuracy: 0.8950617283950617\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.89      0.92      0.90        86\n",
            "       HUMAN       0.90      0.87      0.89        76\n",
            "\n",
            "    accuracy                           0.90       162\n",
            "   macro avg       0.90      0.89      0.89       162\n",
            "weighted avg       0.90      0.90      0.89       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM Output Download"
      ],
      "metadata": {
        "id": "wrv20frWM-Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib  # For loading saved models\n",
        "\n",
        "# Paths to files\n",
        "test_data_path = \"/content/drive/Shareddrives/AI Text Detecting/tamil_test.xlsx\"  # Replace with your test data file path\n",
        "output_tsv_path = \"svm_test_data_with_generated_labels.tsv\"  # Path to save the output TSV file\n",
        "\n",
        "# Load test data\n",
        "test_data = pd.read_excel(test_data_path)  # Ensure your test file has a column with preprocessed text data\n",
        "test_texts = test_data[\"DATA\"]  # Replace \"DATA\" with the column name in your test file\n",
        "\n",
        "# Load the saved TF-IDF Vectorizer and SVM model\n",
        "tfidf_vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")  # Load the fitted vectorizer\n",
        "svm_model = joblib.load(\"svm_tfidf_model.pkl\")          # Load the trained SVM model\n",
        "\n",
        "# Transform the preprocessed test data using the loaded vectorizer\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_texts)\n",
        "\n",
        "# Predict labels for the test data using the loaded SVM model\n",
        "predicted_labels = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Map numerical labels to human-readable labels (adjust based on your trained model's label mapping)\n",
        "#label_mapping = {0: \"Human\", 1: \"AI\"}  # Replace or adjust this mapping if needed\n",
        "#test_data[\"LABEL\"] = [label_mapping[label] for label in predicted_labels]\n",
        "\n",
        "# Map numerical labels to human-readable labels (adjust based on your trained model's label mapping)\n",
        "label_mapping = {0: \"Human\", 1: \"AI\"}  # Replace or adjust this mapping if needed\n",
        "\n",
        "# Convert predicted labels to numerical before using the mapping\n",
        "predicted_labels_numerical = [0 if label == 'HUMAN' else 1 for label in predicted_labels]\n",
        "\n",
        "# Now use the numerical labels for mapping\n",
        "test_data[\"LABEL\"] = [label_mapping[label] for label in predicted_labels_numerical]\n",
        "\n",
        "# Save the test data with generated labels to a TSV file\n",
        "test_data[[\"DATA\", \"LABEL\"]].to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_tsv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNitVdTNFV9Q",
        "outputId": "74a5458b-cb8f-4732-d885-a0bba98af1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to svm_test_data_with_generated_labels.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Paths to files\n",
        "test_data_path = \"/content/drive/Shareddrives/AI Text Detecting/tamil_test.xlsx\"\n",
        "output_tsv_path = \"id_svm_test_data_with_generated_labels.tsv\"\n",
        "\n",
        "# Load test data\n",
        "test_data = pd.read_excel(test_data_path)\n",
        "test_texts = test_data[\"DATA\"]\n",
        "\n",
        "# Load the saved TF-IDF Vectorizer and SVM model\n",
        "tfidf_vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "svm_model = joblib.load(\"svm_tfidf_model.pkl\")\n",
        "\n",
        "# Transform the preprocessed test data using the loaded vectorizer\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_texts)\n",
        "\n",
        "# Predict labels for the test data using the loaded SVM model\n",
        "predicted_labels = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Assign the predicted labels to the test data\n",
        "test_data[\"LABEL\"] = predicted_labels\n",
        "\n",
        "# Save the test data with ID, DATA, and LABEL columns to a TSV file\n",
        "test_data[[\"ID\", \"DATA\", \"LABEL\"]].to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_tsv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGvNNc0frc8V",
        "outputId": "e0c01981-29d6-4265-ea93-6d22942f338b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to id_svm_test_data_with_generated_labels.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "lz7dn2-jQHEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Tamil stop words (You can expand this list as needed)\n",
        "tamil_stopwords = [\n",
        "    'அது', 'இது', 'அவ்', 'அவர்', 'எனது', 'இந்த', 'என்று', 'ஒரு', 'இலை', 'உள்ள', 'இல்', 'எனக்கு', 'என', 'அந்த', 'உங்கள்', 'நான்', 'இங்கு'\n",
        "    # Add more Tamil stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sஅஆஇஈஉஊஎஎஐஒஓக்களளழவமயரலவறனபஙஜஞசடணதபணமோ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in tamil_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# ------------------------------ 1. Random Forest with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_bow = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = rf_bow.predict(X_test_bow)\n",
        "print(\"Random Forest with Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. Random Forest with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = rf_tfidf.predict(X_test_tfidf)\n",
        "print(\"Random Forest with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. Random Forest with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_ngram_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = rf_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"Random Forest with N-gram and TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. Random Forest with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_ngram = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = rf_ngram.predict(X_test_ngram)\n",
        "print(\"Random Forest with Count Vectorization and N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rn50BpqQJPW",
        "outputId": "f38fdcbe-d2f0-4083-deda-f009bdf1369e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Bag of Words - Accuracy: 0.8518518518518519\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.90      0.81      0.85        86\n",
            "       HUMAN       0.81      0.89      0.85        76\n",
            "\n",
            "    accuracy                           0.85       162\n",
            "   macro avg       0.85      0.85      0.85       162\n",
            "weighted avg       0.86      0.85      0.85       162\n",
            "\n",
            "Random Forest with TF-IDF - Accuracy: 0.8580246913580247\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.89      0.84      0.86        86\n",
            "       HUMAN       0.83      0.88      0.85        76\n",
            "\n",
            "    accuracy                           0.86       162\n",
            "   macro avg       0.86      0.86      0.86       162\n",
            "weighted avg       0.86      0.86      0.86       162\n",
            "\n",
            "Random Forest with N-gram and TF-IDF - Accuracy: 0.8580246913580247\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.91      0.81      0.86        86\n",
            "       HUMAN       0.81      0.91      0.86        76\n",
            "\n",
            "    accuracy                           0.86       162\n",
            "   macro avg       0.86      0.86      0.86       162\n",
            "weighted avg       0.86      0.86      0.86       162\n",
            "\n",
            "Random Forest with Count Vectorization and N-gram - Accuracy: 0.8333333333333334\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.89      0.78      0.83        86\n",
            "       HUMAN       0.78      0.89      0.83        76\n",
            "\n",
            "    accuracy                           0.83       162\n",
            "   macro avg       0.84      0.84      0.83       162\n",
            "weighted avg       0.84      0.83      0.83       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "mtlmcvMYQklm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Tamil stop words (You can expand this list as needed)\n",
        "tamil_stopwords = [\n",
        "    'அது', 'இது', 'அவ்', 'அவர்', 'எனது', 'இந்த', 'என்று', 'ஒரு', 'இலை', 'உள்ள', 'இல்', 'எனக்கு', 'என', 'அந்த', 'உங்கள்', 'நான்', 'இங்கு'\n",
        "    # Add more Tamil stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sஅஆஇஈஉஊஎஎஐஒஓக்களளழவமயரலவறனபஙஜஞசடணதபணமோ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in tamil_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# ------------------------------ 1. KNN with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train KNN model\n",
        "knn_bow = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = knn_bow.predict(X_test_bow)\n",
        "print(\"KNN with Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. KNN with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train KNN model\n",
        "knn_tfidf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = knn_tfidf.predict(X_test_tfidf)\n",
        "print(\"KNN with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. KNN with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train KNN model\n",
        "knn_ngram_tfidf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = knn_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"KNN with N-gram and TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. KNN with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train KNN model\n",
        "knn_ngram = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = knn_ngram.predict(X_test_ngram)\n",
        "print(\"KNN with Count Vectorization and N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYjdXdy3QkRo",
        "outputId": "2e646cd2-265e-4682-ff91-c6eb2619d020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with Bag of Words - Accuracy: 0.6604938271604939\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.97      0.37      0.54        86\n",
            "       HUMAN       0.58      0.99      0.73        76\n",
            "\n",
            "    accuracy                           0.66       162\n",
            "   macro avg       0.78      0.68      0.63       162\n",
            "weighted avg       0.79      0.66      0.63       162\n",
            "\n",
            "KNN with TF-IDF - Accuracy: 0.8580246913580247\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.90      0.83      0.86        86\n",
            "       HUMAN       0.82      0.89      0.86        76\n",
            "\n",
            "    accuracy                           0.86       162\n",
            "   macro avg       0.86      0.86      0.86       162\n",
            "weighted avg       0.86      0.86      0.86       162\n",
            "\n",
            "KNN with N-gram and TF-IDF - Accuracy: 0.8580246913580247\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.88      0.85      0.86        86\n",
            "       HUMAN       0.84      0.87      0.85        76\n",
            "\n",
            "    accuracy                           0.86       162\n",
            "   macro avg       0.86      0.86      0.86       162\n",
            "weighted avg       0.86      0.86      0.86       162\n",
            "\n",
            "KNN with Count Vectorization and N-gram - Accuracy: 0.5308641975308642\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.92      0.13      0.22        86\n",
            "       HUMAN       0.50      0.99      0.66        76\n",
            "\n",
            "    accuracy                           0.53       162\n",
            "   macro avg       0.71      0.56      0.44       162\n",
            "weighted avg       0.72      0.53      0.43       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decison Tree"
      ],
      "metadata": {
        "id": "9jNsIHsxQ-vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Tamil stop words (You can expand this list as needed)\n",
        "tamil_stopwords = [\n",
        "    'அது', 'இது', 'அவ்', 'அவர்', 'எனது', 'இந்த', 'என்று', 'ஒரு', 'இலை', 'உள்ள', 'இல்', 'எனக்கு', 'என', 'அந்த', 'உங்கள்', 'நான்', 'இங்கு'\n",
        "    # Add more Tamil stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sஅஆஇஈஉஊஎஎஐஒஓக்களளழவமயரலவறனபஙஜஞசடணதபணமோ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in tamil_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# ------------------------------ 1. Decision Tree with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt_bow = DecisionTreeClassifier(random_state=42)\n",
        "dt_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = dt_bow.predict(X_test_bow)\n",
        "print(\"Decision Tree with Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. Decision Tree with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt_tfidf = DecisionTreeClassifier(random_state=42)\n",
        "dt_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = dt_tfidf.predict(X_test_tfidf)\n",
        "print(\"Decision Tree with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. Decision Tree with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt_ngram_tfidf = DecisionTreeClassifier(random_state=42)\n",
        "dt_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = dt_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"Decision Tree with N-gram and TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. Decision Tree with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt_ngram = DecisionTreeClassifier(random_state=42)\n",
        "dt_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = dt_ngram.predict(X_test_ngram)\n",
        "print(\"Decision Tree with Count Vectorization and N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30dIbpRcRA0z",
        "outputId": "b8eaffc5-6c30-477d-b60d-77e9ec971bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with Bag of Words - Accuracy: 0.8271604938271605\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.88      0.78      0.83        86\n",
            "       HUMAN       0.78      0.88      0.83        76\n",
            "\n",
            "    accuracy                           0.83       162\n",
            "   macro avg       0.83      0.83      0.83       162\n",
            "weighted avg       0.83      0.83      0.83       162\n",
            "\n",
            "Decision Tree with TF-IDF - Accuracy: 0.8148148148148148\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.83      0.81      0.82        86\n",
            "       HUMAN       0.79      0.82      0.81        76\n",
            "\n",
            "    accuracy                           0.81       162\n",
            "   macro avg       0.81      0.81      0.81       162\n",
            "weighted avg       0.82      0.81      0.81       162\n",
            "\n",
            "Decision Tree with N-gram and TF-IDF - Accuracy: 0.8209876543209876\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.87      0.78      0.82        86\n",
            "       HUMAN       0.78      0.87      0.82        76\n",
            "\n",
            "    accuracy                           0.82       162\n",
            "   macro avg       0.82      0.82      0.82       162\n",
            "weighted avg       0.83      0.82      0.82       162\n",
            "\n",
            "Decision Tree with Count Vectorization and N-gram - Accuracy: 0.845679012345679\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.87      0.84      0.85        86\n",
            "       HUMAN       0.82      0.86      0.84        76\n",
            "\n",
            "    accuracy                           0.85       162\n",
            "   macro avg       0.85      0.85      0.85       162\n",
            "weighted avg       0.85      0.85      0.85       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert Model"
      ],
      "metadata": {
        "id": "tEZQPw3BiiD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets torch scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldkuzUYQihvA",
        "outputId": "d60fc065-c006-436d-8803-cd5961dd9354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "processed_data['encoded_label'] = label_encoder.fit_transform(processed_data['LABEL'])\n",
        "\n",
        "# Updated labels\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['encoded_label']  # Encoded labels\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Update the dataset class to use encoded labels\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts.iloc[idx]\n",
        "        label = self.labels.iloc[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)  # Use encoded integer label\n",
        "        }\n",
        "\n",
        "# Example usage for dataset\n",
        "train_dataset = CustomDataset(X_train, y_train, tokenizer, max_length=128)\n",
        "test_dataset = CustomDataset(X_test, y_test, tokenizer, max_length=128)\n"
      ],
      "metadata": {
        "id": "esjUq82ilJZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Tamil stop words (You can expand this list as needed)\n",
        "tamil_stopwords = [\n",
        "    'அது', 'இது', 'அவ்', 'அவர்', 'எனது', 'இந்த', 'என்று', 'ஒரு', 'இலை', 'உள்ள', 'இல்', 'எனக்கு', 'என', 'அந்த', 'உங்கள்', 'நான்', 'இங்கு'\n",
        "    # Add more Tamil stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sஅஆஇஈஉஊஎஎஐஒஓக்களளழவமயரலவறனபஙஜஞசடணதபணமோ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    filtered_tokens = [word for word in tokens if word not in tamil_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['encoded_label'] = label_encoder.fit_transform(df['LABEL'])\n",
        "\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text', 'encoded_label']], label_encoder\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv'\n",
        "\n",
        "# Preprocess the data\n",
        "processed_data, label_encoder = preprocess_data(file_path)\n",
        "\n",
        "# Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['encoded_label']  # Encoded Labels\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts.iloc[idx]\n",
        "        label = self.labels.iloc[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CustomDataset(X_train, y_train, tokenizer, max_length=128)\n",
        "test_dataset = CustomDataset(X_test, y_test, tokenizer, max_length=128)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Define the model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training function\n",
        "def train_model():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['label']\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model():\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['label']\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions, target_names=label_encoder.classes_)\n",
        "    return accuracy, report\n",
        "\n",
        "# Train and evaluate the model\n",
        "EPOCHS = 3\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_model()\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "accuracy, report = evaluate_model()\n",
        "print(\"Evaluation Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj-tP3G0lzi0",
        "outputId": "35fed6b0-65bf-4c27-cc47-1785158fa32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.5683\n",
            "Epoch 2/3, Loss: 0.3095\n",
            "Epoch 3/3, Loss: 0.2352\n",
            "Evaluation Accuracy: 0.8950617283950617\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.90      0.91      0.90        86\n",
            "       HUMAN       0.89      0.88      0.89        76\n",
            "\n",
            "    accuracy                           0.90       162\n",
            "   macro avg       0.89      0.89      0.89       162\n",
            "weighted avg       0.90      0.90      0.90       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas torch transformers scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRYZmK8UStlO",
        "outputId": "e0b13c26-7f11-4d4e-e559-ca9c661a78e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EtTABWkTRuO",
        "outputId": "ebc5e723-419d-40d7-dc99-a16b9d9ed212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCpsHm0aTzKl",
        "outputId": "711f7dc3-a538-4059-8414-d3d1f84858ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2lTPRKmUC_x",
        "outputId": "1e032ff1-d0b9-4730-a8a1-c7808385518b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4W4kcV5HUPab",
        "outputId": "e6975162-37f1-4746-bfdd-5a2e27c69feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              },
              "id": "b200725742204309945a612cd395f66c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/drive/Shareddrives/AI Text Detecting/tamil_dataset.csv\")  # Replace with your dataset's filename\n",
        "reviews = data[\"DATA\"]  # Replace 'DATA' with your column name for reviews\n",
        "labels = data[\"LABEL\"]  # Replace 'LABEL' with your column name for labels\n",
        "\n",
        "# Sample Tamil stop words (expand this list as needed)\n",
        "tamil_stopwords = [\n",
        "    'அது', 'இது', 'அவ்', 'அவர்', 'எனது', 'இந்த', 'என்று', 'ஒரு', 'இலை', 'உள்ள', 'இல்',\n",
        "    'எனக்கு', 'என', 'அந்த', 'உங்கள்', 'நான்', 'இங்கு'\n",
        "]\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess_text(text, stop_words):\n",
        "    # Tokenize and remove stop words\n",
        "    words = text.split()\n",
        "    cleaned_words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(cleaned_words)\n",
        "\n",
        "# Preprocess the reviews\n",
        "data[\"cleaned_review\"] = data[\"DATA\"].apply(lambda x: preprocess_text(x, tamil_stopwords))\n",
        "\n",
        "# Encode labels to integers if not already done\n",
        "label_mapping = {label: idx for idx, label in enumerate(data[\"LABEL\"].unique())}\n",
        "data[\"encoded_label\"] = data[\"LABEL\"].map(label_mapping)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data[\"cleaned_review\"], data[\"encoded_label\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize data using a transformer tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Create custom dataset class\n",
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = ReviewsDataset(train_encodings, list(train_labels))\n",
        "test_dataset = ReviewsDataset(test_encodings, list(test_labels))\n",
        "\n",
        "# Load transformer model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", num_labels=len(label_mapping)\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    report_to = \"none\"\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./tamil_review_model\")\n",
        "tokenizer.save_pretrained(\"./tamil_review_model\")\n",
        "\n",
        "# Optional: Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", results)\n",
        "\n",
        "# Test the model with new data (optional)\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_class = torch.argmax(probs).item()\n",
        "    return {v: k for k, v in label_mapping.items()}[predicted_class]\n",
        "\n",
        "# Example prediction\n",
        "example_review = \"இந்த தயாரிப்பு மிகவும் சிறந்தது\"\n",
        "print(\"Predicted label:\", predict(example_review))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587,
          "referenced_widgets": [
            "26ee2821e3d542229dd1638d12828b1c",
            "a6cea84a42084d199ffc8d65acda3fd4",
            "ea2b131b6c504c3ba731fb02fa21c75b",
            "49c2fdb9f08e42229768ed7969a3295a",
            "8a365541f1bd4efa9ce5e5ad10035bb6",
            "7eee0e44e26b4b2b8c3594e36d13694e",
            "b7af2807b8624f47846677d231e30d87",
            "09aad75c74f148f98502bc7fcca2981f",
            "7b701fac79d54135b170c2360df74202",
            "fc28297c5e5d46f5aa36e443556616de",
            "4815cd41cebb4616b5b35fa1d4d3f175",
            "32ebfb69742b42ab9b66e2c16894f8c4",
            "bc46f55fbb1a42a5aeb70bda3196ca94",
            "d359bb3b8bf14411af51090b81626b6a",
            "d2ba44c5e43443aca094e16b36227b91",
            "b9c025aa13214925a494983492c5bfe3",
            "eece71bfa59d4b5e83d450c213a28f8e",
            "15c3cb0da9e94bcf8a39db551dbbed92",
            "318e5b691d7840a98c48572efc21a45c",
            "3675badaaa4e4e86a7f3f34992909f16",
            "f3c71654b22740ba8401433838c45083",
            "7b2492e19e5440e88dd09478895f94e8",
            "265e64a983064e0d94e44bbe4bbecadf",
            "90aba49bfb5e4ff3bb11bd627f89b20d",
            "f5042a6ca5514830bdb49139c61b4792",
            "a081f9217cd04275bafa10100979776d",
            "777c50fea38d469f811f334adcccc7cc",
            "c5fe60f3922943e9a8d6760c003d8e58",
            "6d2dba46b4024fb6a67ab3f4e5569136",
            "985fbed3bf6249d7b4c8f96b75f4f28a",
            "a303cc59be334d1ca2c31bbe5b76994e",
            "fe2748fe887e48ae994db3b2fcd91fc1",
            "88a3745bfce64bf19e9a0b7b223835e5",
            "e57d71c134a4403f8fb2897c5369a471",
            "e22a53b6082547b38b4a8134251230cc",
            "f3198999fd7245b08a0c1e2b92b2c936",
            "ba3281d8c0f9453cadd3abe2487cbf9d",
            "f9bc2e0027ec47848e689adcd8b18afa",
            "b2ac48e8e10a4a67b1328c598ce75dae",
            "455b89ae304b409e8969e909c90d880f",
            "681ec8af1267439aaf19bb5cccdaf936",
            "c4477bc6f8514a00b6748578574f7d39",
            "6cf5031165154362b0603d8faf3f3cce",
            "01d61b487c8a4ef8a333cef99510d7a2",
            "aca58216961d4689adca2144760294e7",
            "f48371f164424dc795c48a4371795938",
            "8ed377205dcb423c8b1faa86b0cb9a9c",
            "8947201c84a64d989a3c53cd1a245128",
            "8562cb1aa5564549b0cbc14939eeac01",
            "d9b65b3261624a0b9cf7216791080a61",
            "2fd62e63e71b42e7ba7c2230be3ea79b",
            "0b898dbaccec4dfcb1e7072b8060e18c",
            "b68cd27bff934606a920c7bf3b154e46",
            "9b2d6eeafdcc4ea099b74c5b875e612d",
            "bd01fc07699a416a92beb8111fc31441"
          ]
        },
        "id": "DoOECvckWMxn",
        "outputId": "47caf3a6-81b6-4e88-a1d9-7fc73f40e847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26ee2821e3d542229dd1638d12828b1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32ebfb69742b42ab9b66e2c16894f8c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "265e64a983064e0d94e44bbe4bbecadf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e57d71c134a4403f8fb2897c5369a471"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aca58216961d4689adca2144760294e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [123/123 31:44, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.137600</td>\n",
              "      <td>0.045628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.057400</td>\n",
              "      <td>0.232094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.038500</td>\n",
              "      <td>0.100289</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11/11 00:35]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.10028889775276184, 'eval_runtime': 39.1269, 'eval_samples_per_second': 4.14, 'eval_steps_per_second': 0.281, 'epoch': 3.0}\n",
            "Predicted label: HUMAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions and true labels\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "true_labels = test_labels.tolist()\n",
        "class_report = classification_report(\n",
        "    true_labels,\n",
        "    predicted_labels,\n",
        "    target_names=[label for label in label_mapping.keys()],\n",
        "    zero_division=0\n",
        ")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "d1u7PonXfXwp",
        "outputId": "9e453a05-cc02-4756-e36d-0bb3d201b135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.99      0.98      0.98        86\n",
            "       HUMAN       0.97      0.99      0.98        76\n",
            "\n",
            "    accuracy                           0.98       162\n",
            "   macro avg       0.98      0.98      0.98       162\n",
            "weighted avg       0.98      0.98      0.98       162\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model with new data (optional)\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_class = torch.argmax(probs).item()\n",
        "    return {v: k for k, v in label_mapping.items()}[predicted_class]\n",
        "\n",
        "# Example prediction\n",
        "example_review = \"இயல்பான சுவையும் நன்கு விரும்பத்தக்க தரத்தையும் வழங்கும் அருமையான தயாரிப்பு!\"\n",
        "print(\"Predicted label:\", predict(example_review))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5XpOSgkgBbm",
        "outputId": "b57f2f55-c095-43c4-8d88-eff698fe174e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: HUMAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert Output file download"
      ],
      "metadata": {
        "id": "77bLBdqeMT5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Paths to files and model\n",
        "test_data_path = \"/content/drive/Shareddrives/AI Text Detecting/tamil_test.xlsx\"  # Replace with your test data path\n",
        "model_path = \"./tamil_review_model\"  # Path to your trained model\n",
        "output_tsv_path = \"test_data_with_generated_labels.tsv\"  # Path for the output TSV file\n",
        "\n",
        "# Load test data from .xlsx file\n",
        "test_data = pd.read_excel(test_data_path)  # Make sure to adjust this if your file has multiple sheets\n",
        "test_texts = test_data[\"DATA\"]\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Function to make predictions\n",
        "def predict(texts):\n",
        "    predictions = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(probs).item()\n",
        "        predictions.append(predicted_class)\n",
        "    return predictions\n",
        "\n",
        "# Generate predictions\n",
        "predicted_labels = predict(test_texts)\n",
        "\n",
        "# Map predictions to AI or Human (modify as per your trained labels)\n",
        "label_mapping = {0: \"Human\", 1: \"AI\"}  # Adjust based on your trained labels\n",
        "\n",
        "# Create the LABEL column with predictions\n",
        "test_data[\"LABEL\"] = [label_mapping[label] for label in predicted_labels]\n",
        "\n",
        "# Save the results to a TSV file\n",
        "test_data.to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "\n",
        "\n",
        "print(f\"Predictions saved to {output_tsv_path}\")\n",
        "\n",
        "# Print out the first few rows of the resulting DataFrame\n",
        "print(test_data.head())\n",
        "\n",
        "# Optionally, you can generate a classification report if you have ground truth labels later.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfvrnwLDFVWL",
        "outputId": "8a1fc37e-b425-429e-f09e-18185f75d701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to test_data_with_generated_labels.tsv\n",
            "                ID                                               DATA  LABEL\n",
            "0  TAM_HUAI_TE_001   நான் அண்மையில் வாங்கிய ஒரு வாட்டர் பாட்டில் ம...  Human\n",
            "1  TAM_HUAI_TE_002                        அணிவதற்கு நன்றாக இருக்கும்      AI\n",
            "2  TAM_HUAI_TE_003                                  அதிக வாசனை வாந்தி     AI\n",
            "3  TAM_HUAI_TE_004                   அதிகமக பயன்பதுதினல் தலை சுட்ரும்     AI\n",
            "4  TAM_HUAI_TE_005                                    அழகாக இருக்கும்     AI\n"
          ]
        }
      ]
    }
  ]
}