{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a9ebe1ab8874afc8975b4e1d3d37778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2c6a8243ab5418e98e57004a1fa394f",
              "IPY_MODEL_61eab914a5c44375ad22488f3b5f6ca4",
              "IPY_MODEL_421b4547201c4a39af080da731817c88"
            ],
            "layout": "IPY_MODEL_88a20fc0299745f1a19f73bb23d38729"
          }
        },
        "e2c6a8243ab5418e98e57004a1fa394f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcc0173ea9ea4af3bb94f1c3be8a15ce",
            "placeholder": "​",
            "style": "IPY_MODEL_6b063bc0d78d45f68d82b455f3867463",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "61eab914a5c44375ad22488f3b5f6ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e61b35706a4140ddbc276bb3fec194c2",
            "max": 49,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4afe7d1912f402d8db57163fbea9115",
            "value": 49
          }
        },
        "421b4547201c4a39af080da731817c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88b00f9141e74c3e8119a3e0314cfd2a",
            "placeholder": "​",
            "style": "IPY_MODEL_bc71094fde0d4a41af67a0e62ec22fe4",
            "value": " 49.0/49.0 [00:00&lt;00:00, 1.17kB/s]"
          }
        },
        "88a20fc0299745f1a19f73bb23d38729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcc0173ea9ea4af3bb94f1c3be8a15ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b063bc0d78d45f68d82b455f3867463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e61b35706a4140ddbc276bb3fec194c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4afe7d1912f402d8db57163fbea9115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88b00f9141e74c3e8119a3e0314cfd2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc71094fde0d4a41af67a0e62ec22fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11fa87899ae74d978ba63cb520e34179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7047096c2a2e4de6bd952a6e03a2152e",
              "IPY_MODEL_831b64562d014e3389833e5dbd108bf1",
              "IPY_MODEL_86e309c5437744518aec97a11e45d7cf"
            ],
            "layout": "IPY_MODEL_795b219b7bc041d7bbb32f368c6822f3"
          }
        },
        "7047096c2a2e4de6bd952a6e03a2152e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_305788c9261d40589cdf080d5ab7db1a",
            "placeholder": "​",
            "style": "IPY_MODEL_90bae68c293e43a4b0e019e4e5d6781f",
            "value": "config.json: 100%"
          }
        },
        "831b64562d014e3389833e5dbd108bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_018f0404108e4ebf8573de49c77f6f9f",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab5902cfef7843c7809c00bce36be671",
            "value": 625
          }
        },
        "86e309c5437744518aec97a11e45d7cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c199e18efb2d4758a50103f606c3a8a8",
            "placeholder": "​",
            "style": "IPY_MODEL_f4d435ba082b4e3ea80c282b5d0c1ae4",
            "value": " 625/625 [00:00&lt;00:00, 25.5kB/s]"
          }
        },
        "795b219b7bc041d7bbb32f368c6822f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "305788c9261d40589cdf080d5ab7db1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90bae68c293e43a4b0e019e4e5d6781f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "018f0404108e4ebf8573de49c77f6f9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab5902cfef7843c7809c00bce36be671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c199e18efb2d4758a50103f606c3a8a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4d435ba082b4e3ea80c282b5d0c1ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b347821fa2a4adcb9e9cafd36584542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81e63b2ee3104a2798ed06bfaaf9e36e",
              "IPY_MODEL_6fd26e1c76d64861b1cc56b6b558e589",
              "IPY_MODEL_5e378c2724184eb28fadc4e9d0db39b9"
            ],
            "layout": "IPY_MODEL_6579dfe512954eebb1cae8df81321ac0"
          }
        },
        "81e63b2ee3104a2798ed06bfaaf9e36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd06ace1a2b14c45b5f5cd144f17d412",
            "placeholder": "​",
            "style": "IPY_MODEL_f4e1c32f6b6740a0acf5f0889db5567a",
            "value": "vocab.txt: 100%"
          }
        },
        "6fd26e1c76d64861b1cc56b6b558e589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf0f02d5bdc844e5b2e83dfbd212858d",
            "max": 995526,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bf8849a22a64810b2fe6af093371ed6",
            "value": 995526
          }
        },
        "5e378c2724184eb28fadc4e9d0db39b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cad6ee82c16b40e489f38a64dff7f5c3",
            "placeholder": "​",
            "style": "IPY_MODEL_eb3ae7ac618d41e68c15b693d554fc6d",
            "value": " 996k/996k [00:00&lt;00:00, 5.82MB/s]"
          }
        },
        "6579dfe512954eebb1cae8df81321ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd06ace1a2b14c45b5f5cd144f17d412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4e1c32f6b6740a0acf5f0889db5567a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf0f02d5bdc844e5b2e83dfbd212858d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bf8849a22a64810b2fe6af093371ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cad6ee82c16b40e489f38a64dff7f5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb3ae7ac618d41e68c15b693d554fc6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8525ab92e734a8e9679bd873d100b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b546f57062649ffbd25758f1bb969b0",
              "IPY_MODEL_e3fbc8fad21a47c4bffbf7cdd1744d3a",
              "IPY_MODEL_449de2e5bd9142788c177d44d9ea7e9d"
            ],
            "layout": "IPY_MODEL_632cc56a4e084df5b4ad249c1a573b72"
          }
        },
        "9b546f57062649ffbd25758f1bb969b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8664ce7e8e51435587b915cff1c1b4b8",
            "placeholder": "​",
            "style": "IPY_MODEL_238ab95a086e49a7bdd5d2561fdd27bf",
            "value": "tokenizer.json: 100%"
          }
        },
        "e3fbc8fad21a47c4bffbf7cdd1744d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca103bd950574fc9b4bafdcbcf6b1cb5",
            "max": 1961828,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5754825a35a4585aec95732015a670d",
            "value": 1961828
          }
        },
        "449de2e5bd9142788c177d44d9ea7e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2f5e198c69a4cfaa84006a6c38e2900",
            "placeholder": "​",
            "style": "IPY_MODEL_a268308fde9a49a787ee645c2f3547df",
            "value": " 1.96M/1.96M [00:00&lt;00:00, 30.6MB/s]"
          }
        },
        "632cc56a4e084df5b4ad249c1a573b72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8664ce7e8e51435587b915cff1c1b4b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "238ab95a086e49a7bdd5d2561fdd27bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca103bd950574fc9b4bafdcbcf6b1cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5754825a35a4585aec95732015a670d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2f5e198c69a4cfaa84006a6c38e2900": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a268308fde9a49a787ee645c2f3547df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89e3d1e22d9e41b98856eece6ed7db20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c07bd21918a41fb96d781783e3e3ed5",
              "IPY_MODEL_106e373f66824013b827b09de5c937b6",
              "IPY_MODEL_24bd91783a6341f5bdf3ed97d99eeee3"
            ],
            "layout": "IPY_MODEL_bb8346910f6342d382827c69a46276fe"
          }
        },
        "9c07bd21918a41fb96d781783e3e3ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d35f37b458a43759a6c4ecdf46fffc6",
            "placeholder": "​",
            "style": "IPY_MODEL_33ac727e63434a54a75d8bdcf358f5a1",
            "value": "model.safetensors: 100%"
          }
        },
        "106e373f66824013b827b09de5c937b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63319dc15cd1412ebb0db8505ced66a5",
            "max": 714290682,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fce04d01bed04a2abb495830f1ad4c59",
            "value": 714290682
          }
        },
        "24bd91783a6341f5bdf3ed97d99eeee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6f1edab71d14f3f98c0f14929e1069a",
            "placeholder": "​",
            "style": "IPY_MODEL_debed1b5caa744bbb26857f25f02f76a",
            "value": " 714M/714M [00:05&lt;00:00, 188MB/s]"
          }
        },
        "bb8346910f6342d382827c69a46276fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d35f37b458a43759a6c4ecdf46fffc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33ac727e63434a54a75d8bdcf358f5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63319dc15cd1412ebb0db8505ced66a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fce04d01bed04a2abb495830f1ad4c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6f1edab71d14f3f98c0f14929e1069a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "debed1b5caa744bbb26857f25f02f76a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive"
      ],
      "metadata": {
        "id": "m6tT4Ls0Pcsz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chJcjTN-yxhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e573fe-7c0a-470c-c6c4-0458832b990c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "#file path\n",
        "tamil_dataset = '/content/drive/Shareddrives/AI Text Detecting/malayalam_dataset.csv'\n",
        "\n",
        "\n",
        "# Step 3: Load the Dataset\n",
        "# Assuming it's a CSV file\n",
        "dataset = pd.read_csv(tamil_dataset)\n",
        "\n",
        "\n",
        "# Step 4: Display the Dataset\n",
        "# Show the first few rows\n",
        "print(dataset.head())\n",
        "\n",
        "print()\n",
        "# Show information about the dataset\n",
        "print(dataset.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8aGGNhCNWIp",
        "outputId": "cf9aee68-1243-447e-9294-801be5503f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                ID                                               DATA  LABEL\n",
            "0  MAL_HUAI_TR_001  ഞാൻ കുറച്ച് കാലമായി മുച്ചട്ച്ചിൻ്റെ ഫേസ് വാഷ് ...  HUMAN\n",
            "1  MAL_HUAI_TR_002           ഈ ഫേസ് വാഷ് തണുപ്പ് വെതറിലും ഉപയോഗിക്കാം  HUMAN\n",
            "2  MAL_HUAI_TR_003  അണ്ണാ എനിക്ക് 14 വയസ് ആയ തേയോളു എനിക്ക് സ്കിൻക...  HUMAN\n",
            "3  MAL_HUAI_TR_004  ബ്രോ ഇതെല്ലം യൂസ്  ആക്കീട്ട് നൈറ്റ് പിന്നെ വേറ...  HUMAN\n",
            "4  MAL_HUAI_TR_005    ഇത് ഫേസ് വാഷ് ഡെയിലി ചെയ്താ സ്കിൻകെയറിന് നല്ലതാ  HUMAN\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 800 entries, 0 to 799\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   ID      800 non-null    object\n",
            " 1   DATA    800 non-null    object\n",
            " 2   LABEL   800 non-null    object\n",
            "dtypes: object(3)\n",
            "memory usage: 18.9+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing and Split Up"
      ],
      "metadata": {
        "id": "CfVb0Z5iPoaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PreProcessing and Split Up\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Malayalam stop words (You can expand this list as needed)\n",
        "malayalam_stopwords = [\n",
        "    'അത്', 'ഇത്', 'അവൻ', 'അവൾ', 'അവ', 'ഞാൻ', 'ഞങ്ങളുടെ', 'ഈ', 'എന്ന', 'ഒരു', 'ഇവൻ', 'ഇവൾ', 'അത്', 'അതിൽ', 'എന്റെ', 'ആ', 'നിങ്ങളുടെ', 'അവിടം'\n",
        "    # Add more Malayalam stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sഅആഇഈഉഊഋഎഏഐഒഓഔകഖഗഘങചഛജഝഞടഠഡഢണതഥദധനപഫബഭമയരറലളഴവശഷസഹ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in malayalam_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/malayalam_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the splits for further processing\n",
        "print(\"Data preprocessing and splitting completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4VmUG5xNv9Q",
        "outputId": "bd156895-9c1f-453d-dc66-14164ec20a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing and splitting completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "-pr9-otGVH0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression with feature extraction\n"
      ],
      "metadata": {
        "id": "79VVTFHVVK1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Malayalam stop words (You can expand this list as needed)\n",
        "malayalam_stopwords = [\n",
        "    'ആ', 'ഇ', 'ഈ', 'എ', 'എന്ത്', 'ഞാൻ', 'ഒരു', 'ഇത്', 'അതിന്റെ', 'അവന്റെ', 'അവളുടെ', 'അത്', 'ആരാ', 'എവിടെയാണ്', 'ഉണ്ട്', 'ഉള്ള', 'അല്ല', 'പോലെ'\n",
        "    # Add more Malayalam stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression (including Malayalam-specific characters)\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sഅആഇഈഉഊഎഏഐഒഓകഖഗഘങചഛജഝഞടഠഡഢണതഥദധനപഫബഭമയരലവശഷസഹളഴറ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in malayalam_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/malayalam_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ------------------------------ 1. Logistic Regression with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logreg_bow = LogisticRegression(max_iter=1000)\n",
        "logreg_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = logreg_bow.predict(X_test_bow)\n",
        "print(\"Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. Logistic Regression with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logreg_tfidf = LogisticRegression(max_iter=1000)\n",
        "logreg_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = logreg_tfidf.predict(X_test_tfidf)\n",
        "print(\"TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. Logistic Regression with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logreg_ngram_tfidf = LogisticRegression(max_iter=1000)\n",
        "logreg_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = logreg_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"N-gram with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. Logistic Regression with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logreg_ngram = LogisticRegression(max_iter=1000)\n",
        "logreg_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = logreg_ngram.predict(X_test_ngram)\n",
        "print(\"Count Vectorization with N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcyiCoDTVNrA",
        "outputId": "a33e2c04-9ff3-48c4-c52f-deccb7e7bc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words - Accuracy: 0.7625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.73      0.84      0.78        80\n",
            "       HUMAN       0.81      0.69      0.74        80\n",
            "\n",
            "    accuracy                           0.76       160\n",
            "   macro avg       0.77      0.76      0.76       160\n",
            "weighted avg       0.77      0.76      0.76       160\n",
            "\n",
            "TF-IDF - Accuracy: 0.75625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.75      0.76      0.76        80\n",
            "       HUMAN       0.76      0.75      0.75        80\n",
            "\n",
            "    accuracy                           0.76       160\n",
            "   macro avg       0.76      0.76      0.76       160\n",
            "weighted avg       0.76      0.76      0.76       160\n",
            "\n",
            "N-gram with TF-IDF - Accuracy: 0.76875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.78      0.75      0.76        80\n",
            "       HUMAN       0.76      0.79      0.77        80\n",
            "\n",
            "    accuracy                           0.77       160\n",
            "   macro avg       0.77      0.77      0.77       160\n",
            "weighted avg       0.77      0.77      0.77       160\n",
            "\n",
            "Count Vectorization with N-gram - Accuracy: 0.76875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.74      0.84      0.78        80\n",
            "       HUMAN       0.81      0.70      0.75        80\n",
            "\n",
            "    accuracy                           0.77       160\n",
            "   macro avg       0.77      0.77      0.77       160\n",
            "weighted avg       0.77      0.77      0.77       160\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM with feature extraction"
      ],
      "metadata": {
        "id": "eYfpskoMWj7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Malayalam stop words (You can expand this list as needed)\n",
        "malayalam_stopwords = [\n",
        "    'ആ', 'ഇ', 'ഈ', 'എ', 'എന്ത്', 'ഞാൻ', 'ഒരു', 'ഇത്', 'അതിന്റെ', 'അവന്റെ', 'അവളുടെ', 'അത്', 'ആരാ', 'എവിടെയാണ്', 'ഉണ്ട്', 'ഉള്ള', 'അല്ല', 'പോലെ'\n",
        "    # Add more Malayalam stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression (including Malayalam-specific characters)\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sഅആഇഈഉഊഎഏഐഒഓകഖഗഘങചഛജഝഞടഠഡഢണതഥദധനപഫബഭമയരലവശഷസഹളഴറ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in malayalam_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/malayalam_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# ------------------------------ 1. SVM with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM model\n",
        "svm_bow = SVC(kernel='linear')\n",
        "svm_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = svm_bow.predict(X_test_bow)\n",
        "print(\"SVM with Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. SVM with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM model\n",
        "svm_tfidf = SVC(kernel='linear')\n",
        "svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
        "print(\"SVM with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. SVM with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM model\n",
        "svm_ngram_tfidf = SVC(kernel='linear')\n",
        "svm_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = svm_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"SVM with N-gram and TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. SVM with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM model\n",
        "svm_ngram = SVC(kernel='linear')\n",
        "svm_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = svm_ngram.predict(X_test_ngram)\n",
        "print(\"SVM with Count Vectorization and N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-HEhxXAWmqy",
        "outputId": "5b34a4a1-9d0a-4527-8784-05e8f5b1a33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM with Bag of Words - Accuracy: 0.69375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.68      0.74      0.71        80\n",
            "       HUMAN       0.71      0.65      0.68        80\n",
            "\n",
            "    accuracy                           0.69       160\n",
            "   macro avg       0.70      0.69      0.69       160\n",
            "weighted avg       0.70      0.69      0.69       160\n",
            "\n",
            "SVM with TF-IDF - Accuracy: 0.725\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.72      0.74      0.73        80\n",
            "       HUMAN       0.73      0.71      0.72        80\n",
            "\n",
            "    accuracy                           0.72       160\n",
            "   macro avg       0.73      0.73      0.72       160\n",
            "weighted avg       0.73      0.72      0.72       160\n",
            "\n",
            "SVM with N-gram and TF-IDF - Accuracy: 0.75625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.76      0.75      0.75        80\n",
            "       HUMAN       0.75      0.76      0.76        80\n",
            "\n",
            "    accuracy                           0.76       160\n",
            "   macro avg       0.76      0.76      0.76       160\n",
            "weighted avg       0.76      0.76      0.76       160\n",
            "\n",
            "SVM with Count Vectorization and N-gram - Accuracy: 0.71875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.71      0.74      0.72        80\n",
            "       HUMAN       0.73      0.70      0.71        80\n",
            "\n",
            "    accuracy                           0.72       160\n",
            "   macro avg       0.72      0.72      0.72       160\n",
            "weighted avg       0.72      0.72      0.72       160\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest with feature extraction"
      ],
      "metadata": {
        "id": "LfPSNMqbXRST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Malayalam stop words (You can expand this list as needed)\n",
        "malayalam_stopwords = [\n",
        "    'ആ', 'ഇ', 'ഈ', 'എ', 'എന്ത്', 'ഞാൻ', 'ഒരു', 'ഇത്', 'അതിന്റെ', 'അവന്റെ', 'അവളുടെ', 'അത്', 'ആരാ', 'എവിടെയാണ്', 'ഉണ്ട്', 'ഉള്ള', 'അല്ല', 'പോലെ'\n",
        "    # Add more Malayalam stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression (including Malayalam-specific characters)\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sഅആഇഈഉഊഎഏഐഒഓകഖഗഘങചഛജഝഞടഠഡഢണതഥദധനപഫബഭമയരലവശഷസഹളഴറ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in malayalam_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/malayalam_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# ------------------------------ 1. Random Forest with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_bow = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = rf_bow.predict(X_test_bow)\n",
        "print(\"Random Forest with Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. Random Forest with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = rf_tfidf.predict(X_test_tfidf)\n",
        "print(\"Random Forest with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. Random Forest with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_ngram_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = rf_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"Random Forest with N-gram and TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. Random Forest with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_ngram = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = rf_ngram.predict(X_test_ngram)\n",
        "print(\"Random Forest with Count Vectorization and N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kWbh1xsXUiE",
        "outputId": "603498b1-4e6d-4205-f145-21064322c7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Bag of Words - Accuracy: 0.75625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.70      0.91      0.79        80\n",
            "       HUMAN       0.87      0.60      0.71        80\n",
            "\n",
            "    accuracy                           0.76       160\n",
            "   macro avg       0.78      0.76      0.75       160\n",
            "weighted avg       0.78      0.76      0.75       160\n",
            "\n",
            "Random Forest with TF-IDF - Accuracy: 0.7875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.77      0.82      0.80        80\n",
            "       HUMAN       0.81      0.75      0.78        80\n",
            "\n",
            "    accuracy                           0.79       160\n",
            "   macro avg       0.79      0.79      0.79       160\n",
            "weighted avg       0.79      0.79      0.79       160\n",
            "\n",
            "Random Forest with N-gram and TF-IDF - Accuracy: 0.75\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.79      0.69      0.73        80\n",
            "       HUMAN       0.72      0.81      0.76        80\n",
            "\n",
            "    accuracy                           0.75       160\n",
            "   macro avg       0.75      0.75      0.75       160\n",
            "weighted avg       0.75      0.75      0.75       160\n",
            "\n",
            "Random Forest with Count Vectorization and N-gram - Accuracy: 0.74375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.70      0.86      0.77        80\n",
            "       HUMAN       0.82      0.62      0.71        80\n",
            "\n",
            "    accuracy                           0.74       160\n",
            "   macro avg       0.76      0.74      0.74       160\n",
            "weighted avg       0.76      0.74      0.74       160\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction"
      ],
      "metadata": {
        "id": "1mo385GjSQ6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Path to the dataset\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/mal_test_data_hum_ai.xlsx'  # Dataset path\n",
        "output_tsv_path = '/content/drive/Shareddrives/AI Text Detecting/malayalam_test_data_with_RFgenerated_labels.tsv'\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "try:\n",
        "    df = pd.read_excel(file_path)  # Load Excel file\n",
        "except UnicodeDecodeError:\n",
        "    df = pd.read_csv(file_path, encoding='latin1')  # Fallback for non-UTF-8 encoded CSV files\n",
        "\n",
        "# Debugging: Print column names\n",
        "print(\"Columns in dataset:\", df.columns)\n",
        "\n",
        "# Step 2: Process the dataset\n",
        "if 'DATA' in df.columns:\n",
        "    X = df['DATA']  # Use 'DATA' for features\n",
        "    # Assign \"Human\" and \"AI\" labels as an example (replace with actual label logic if available)\n",
        "    y = ['Human' if i % 2 == 0 else 'AI' for i in range(len(X))]  # Example: Alternate between Human and AI\n",
        "else:\n",
        "    raise ValueError(\"The dataset must contain a 'DATA' column for text data.\")\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train.astype(str))  # Convert to string\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test.astype(str))\n",
        "\n",
        "# Step 5: Train the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "y_pred = rf_model.predict(X_test_tfidf)\n",
        "print(\"Random Forest with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 7: Predict labels for the entire dataset\n",
        "all_data_tfidf = tfidf_vectorizer.transform(X.astype(str))\n",
        "predicted_labels = rf_model.predict(all_data_tfidf)\n",
        "\n",
        "# Step 8: Add predicted labels to the dataset\n",
        "df['PREDICTED_LABEL'] = predicted_labels\n",
        "\n",
        "# Step 9: Save the results to a TSV file\n",
        "df.to_csv(output_tsv_path, sep='\\t', index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_tsv_path}\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtaYED0p2DJ0",
        "outputId": "15a396bb-d41d-4d89-c8d7-50d5de7ece87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in dataset: Index(['ID', 'DATA'], dtype='object')\n",
            "Random Forest with TF-IDF - Accuracy: 0.45\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.39      0.69      0.50        16\n",
            "       Human       0.58      0.29      0.39        24\n",
            "\n",
            "    accuracy                           0.45        40\n",
            "   macro avg       0.49      0.49      0.44        40\n",
            "weighted avg       0.51      0.45      0.43        40\n",
            "\n",
            "Predictions saved to /content/drive/Shareddrives/AI Text Detecting/malayalam_test_data_with_RFgenerated_labels.tsv\n",
            "                ID                                               DATA  \\\n",
            "0  MAL_HUAI_TE_001  എല്ലാം നന്നായിട്ടുണ്ട് പക്ഷെ എനിക്ക് മീനിന്റെ ...   \n",
            "1  MAL_HUAI_TE_002  ബിലാൽ തട്ടുകടയിൽ നിന്നും പഴകിയ ഫുഡ് കുറെ പ്രാവ...   \n",
            "2  MAL_HUAI_TE_003  കോവയ്ക്ക ഉപ്പിലിട്ടത് എൻറെ ജീവിതത്തിൽ ഇതുവരെ ക...   \n",
            "3  MAL_HUAI_TE_004  കേരളത്തിൽ ഏറ്റവും നല്ല ഭക്ഷണം കിട്ടുന്നത് കോഴി...   \n",
            "4  MAL_HUAI_TE_005  എല്ലാം അടിപൊളി, പക്ഷെ സുരേന്ദ്രൻ തട്ടുകടയിൽ തി...   \n",
            "\n",
            "  PREDICTED_LABEL  \n",
            "0           Human  \n",
            "1              AI  \n",
            "2           Human  \n",
            "3              AI  \n",
            "4           Human  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "FLRqTCUmYCUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Malayalam stop words (You can expand this list as needed)\n",
        "malayalam_stopwords = [\n",
        "    'ആ', 'ഇ', 'ഈ', 'എ', 'എന്ത്', 'ഞാൻ', 'ഒരു', 'ഇത്', 'അതിന്റെ', 'അവന്റെ', 'അവളുടെ', 'അത്', 'ആരാ', 'എവിടെയാണ്', 'ഉണ്ട്', 'ഉള്ള', 'അല്ല', 'പോലെ'\n",
        "    # Add more Malayalam stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression (including Malayalam-specific characters)\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sഅആഇഈഉഊഎഏഐഒഓകഖഗഘങചഛജഝഞടഠഡഢണതഥദധനപഫബഭമയരലവശഷസഹളഴറ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in malayalam_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/malayalam_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# ------------------------------ 1. KNN with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train KNN model\n",
        "knn_bow = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = knn_bow.predict(X_test_bow)\n",
        "print(\"KNN with Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. KNN with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train KNN model\n",
        "knn_tfidf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = knn_tfidf.predict(X_test_tfidf)\n",
        "print(\"KNN with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. KNN with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train KNN model\n",
        "knn_ngram_tfidf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = knn_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"KNN with N-gram and TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. KNN with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train KNN model\n",
        "knn_ngram = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = knn_ngram.predict(X_test_ngram)\n",
        "print(\"KNN with Count Vectorization and N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FNhheZMYDlM",
        "outputId": "5aab90ef-6f8c-4c79-dd77-340ee7a6ae3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with Bag of Words - Accuracy: 0.51875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.67      0.07      0.13        80\n",
            "       HUMAN       0.51      0.96      0.67        80\n",
            "\n",
            "    accuracy                           0.52       160\n",
            "   macro avg       0.59      0.52      0.40       160\n",
            "weighted avg       0.59      0.52      0.40       160\n",
            "\n",
            "KNN with TF-IDF - Accuracy: 0.73125\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.72      0.76      0.74        80\n",
            "       HUMAN       0.75      0.70      0.72        80\n",
            "\n",
            "    accuracy                           0.73       160\n",
            "   macro avg       0.73      0.73      0.73       160\n",
            "weighted avg       0.73      0.73      0.73       160\n",
            "\n",
            "KNN with N-gram and TF-IDF - Accuracy: 0.7125\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.71      0.71      0.71        80\n",
            "       HUMAN       0.71      0.71      0.71        80\n",
            "\n",
            "    accuracy                           0.71       160\n",
            "   macro avg       0.71      0.71      0.71       160\n",
            "weighted avg       0.71      0.71      0.71       160\n",
            "\n",
            "KNN with Count Vectorization and N-gram - Accuracy: 0.525\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       1.00      0.05      0.10        80\n",
            "       HUMAN       0.51      1.00      0.68        80\n",
            "\n",
            "    accuracy                           0.53       160\n",
            "   macro avg       0.76      0.53      0.39       160\n",
            "weighted avg       0.76      0.53      0.39       160\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree with feature extraction"
      ],
      "metadata": {
        "id": "wdUbEJkWYQGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Malayalam stop words (You can expand this list as needed)\n",
        "malayalam_stopwords = [\n",
        "    'ആ', 'ഇ', 'ഈ', 'എ', 'എന്ത്', 'ഞാൻ', 'ഒരു', 'ഇത്', 'അതിന്റെ', 'അവന്റെ', 'അവളുടെ', 'അത്', 'ആരാ', 'എവിടെയാണ്', 'ഉണ്ട്', 'ഉള്ള', 'അല്ല', 'പോലെ'\n",
        "    # Add more Malayalam stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression (including Malayalam-specific characters)\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sഅആഇഈഉഊഎഏഐഒഓകഖഗഘങചഛജഝഞടഠഡഢണതഥദധനപഫബഭമയരലവശഷസഹളഴറ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in malayalam_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/malayalam_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# ------------------------------ 1. Decision Tree with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt_bow = DecisionTreeClassifier(random_state=42)\n",
        "dt_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = dt_bow.predict(X_test_bow)\n",
        "print(\"Decision Tree with Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. Decision Tree with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt_tfidf = DecisionTreeClassifier(random_state=42)\n",
        "dt_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = dt_tfidf.predict(X_test_tfidf)\n",
        "print(\"Decision Tree with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. Decision Tree with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt_ngram_tfidf = DecisionTreeClassifier(random_state=42)\n",
        "dt_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = dt_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"Decision Tree with N-gram and TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. Decision Tree with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt_ngram = DecisionTreeClassifier(random_state=42)\n",
        "dt_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = dt_ngram.predict(X_test_ngram)\n",
        "print(\"Decision Tree with Count Vectorization and N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKq-PcXzYWJ-",
        "outputId": "e719f6e1-0502-4d5c-c1cc-3c62efb6d5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with Bag of Words - Accuracy: 0.73125\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.69      0.84      0.76        80\n",
            "       HUMAN       0.79      0.62      0.70        80\n",
            "\n",
            "    accuracy                           0.73       160\n",
            "   macro avg       0.74      0.73      0.73       160\n",
            "weighted avg       0.74      0.73      0.73       160\n",
            "\n",
            "Decision Tree with TF-IDF - Accuracy: 0.725\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.71      0.75      0.73        80\n",
            "       HUMAN       0.74      0.70      0.72        80\n",
            "\n",
            "    accuracy                           0.72       160\n",
            "   macro avg       0.73      0.72      0.72       160\n",
            "weighted avg       0.73      0.72      0.72       160\n",
            "\n",
            "Decision Tree with N-gram and TF-IDF - Accuracy: 0.69375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.70      0.69      0.69        80\n",
            "       HUMAN       0.69      0.70      0.70        80\n",
            "\n",
            "    accuracy                           0.69       160\n",
            "   macro avg       0.69      0.69      0.69       160\n",
            "weighted avg       0.69      0.69      0.69       160\n",
            "\n",
            "Decision Tree with Count Vectorization and N-gram - Accuracy: 0.7375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.71      0.81      0.76        80\n",
            "       HUMAN       0.78      0.66      0.72        80\n",
            "\n",
            "    accuracy                           0.74       160\n",
            "   macro avg       0.74      0.74      0.74       160\n",
            "weighted avg       0.74      0.74      0.74       160\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes with feature extraction"
      ],
      "metadata": {
        "id": "2U8few-oY-46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample Malayalam stop words (You can expand this list as needed)\n",
        "malayalam_stopwords = [\n",
        "    'ആ', 'ഇ', 'ഈ', 'എ', 'എന്ത്', 'ഞാൻ', 'ഒരു', 'ഇത്', 'അതിന്റെ', 'അവന്റെ', 'അവളുടെ', 'അത്', 'ആരാ', 'എവിടെയാണ്', 'ഉണ്ട്', 'ഉള്ള', 'അല്ല', 'പോലെ'\n",
        "    # Add more Malayalam stop words here\n",
        "]\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuation(text_data):\n",
        "    # Removing punctuation using regular expression (including Malayalam-specific characters)\n",
        "    punctuation_removed = re.sub(r'[^\\w\\sഅആഇഈഉഊഎഏഐഒഓകഖഗഘങചഛജഝഞടഠഡഢണതഥദധനപഫബഭമയരലവശഷസഹളഴറ]', '', text_data)\n",
        "    return punctuation_removed\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenization(text_data):\n",
        "    # Splitting the sentence into words where space is found\n",
        "    tokens_text = re.split(' ', text_data)\n",
        "    return tokens_text\n",
        "\n",
        "# Function for removing stop words\n",
        "def remove_stopwords(tokens):\n",
        "    # Removing stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in malayalam_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(file_path):\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Apply preprocessing to the 'DATA' column (replace with actual column name if different)\n",
        "    # Removing punctuation\n",
        "    df['cleaned_data'] = df['DATA'].apply(remove_punctuation)\n",
        "\n",
        "    # Tokenizing the cleaned data\n",
        "    df['tokenized_data'] = df['cleaned_data'].apply(tokenization)\n",
        "\n",
        "    # Removing stop words\n",
        "    df['filtered_data'] = df['tokenized_data'].apply(remove_stopwords)\n",
        "\n",
        "    # Step 3: Combine tokens back into a single string for each document\n",
        "    df['processed_text'] = df['filtered_data'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Step 4: Return the processed data\n",
        "    return df[['ID', 'DATA', 'LABEL', 'processed_text']]\n",
        "\n",
        "# Path to the dataset (update this with your actual dataset path)\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/malayalam_dataset.csv'  # Adjust the path to your dataset\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "processed_data = preprocess_data(file_path)\n",
        "\n",
        "# Step 6: Prepare the data for model training\n",
        "X = processed_data['processed_text']  # Features\n",
        "y = processed_data['LABEL']  # Labels\n",
        "\n",
        "# Step 7: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# ------------------------------ 1. Naive Bayes with Bag of Words ------------------------------\n",
        "\n",
        "# Bag of Words Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb_bow = MultinomialNB()\n",
        "nb_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = nb_bow.predict(X_test_bow)\n",
        "print(\"Naive Bayes with Bag of Words - Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "# ------------------------------ 2. Naive Bayes with TF-IDF ------------------------------\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb_tfidf = MultinomialNB()\n",
        "nb_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
        "print(\"Naive Bayes with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 3. Naive Bayes with N-gram and TF-IDF ------------------------------\n",
        "\n",
        "# N-gram and TF-IDF Feature Extraction\n",
        "ngram_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram_tfidf = ngram_tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram_tfidf = ngram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb_ngram_tfidf = MultinomialNB()\n",
        "nb_ngram_tfidf.fit(X_train_ngram_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram_tfidf = nb_ngram_tfidf.predict(X_test_ngram_tfidf)\n",
        "print(\"Naive Bayes with N-gram and TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred_ngram_tfidf))\n",
        "print(classification_report(y_test, y_pred_ngram_tfidf))\n",
        "\n",
        "\n",
        "# ------------------------------ 4. Naive Bayes with Count Vectorization and N-gram ------------------------------\n",
        "\n",
        "# Count Vectorization with N-gram Feature Extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "nb_ngram = MultinomialNB()\n",
        "nb_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ngram = nb_ngram.predict(X_test_ngram)\n",
        "print(\"Naive Bayes with Count Vectorization and N-gram - Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
        "print(classification_report(y_test, y_pred_ngram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liuR5lVFZED4",
        "outputId": "9701a5cc-68a9-433a-f0eb-a845f5e4d7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes with Bag of Words - Accuracy: 0.7625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.76      0.76      0.76        80\n",
            "       HUMAN       0.76      0.76      0.76        80\n",
            "\n",
            "    accuracy                           0.76       160\n",
            "   macro avg       0.76      0.76      0.76       160\n",
            "weighted avg       0.76      0.76      0.76       160\n",
            "\n",
            "Naive Bayes with TF-IDF - Accuracy: 0.75\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.74      0.76      0.75        80\n",
            "       HUMAN       0.76      0.74      0.75        80\n",
            "\n",
            "    accuracy                           0.75       160\n",
            "   macro avg       0.75      0.75      0.75       160\n",
            "weighted avg       0.75      0.75      0.75       160\n",
            "\n",
            "Naive Bayes with N-gram and TF-IDF - Accuracy: 0.7625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.76      0.78      0.77        80\n",
            "       HUMAN       0.77      0.75      0.76        80\n",
            "\n",
            "    accuracy                           0.76       160\n",
            "   macro avg       0.76      0.76      0.76       160\n",
            "weighted avg       0.76      0.76      0.76       160\n",
            "\n",
            "Naive Bayes with Count Vectorization and N-gram - Accuracy: 0.76875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.77      0.76      0.77        80\n",
            "       HUMAN       0.77      0.78      0.77        80\n",
            "\n",
            "    accuracy                           0.77       160\n",
            "   macro avg       0.77      0.77      0.77       160\n",
            "weighted avg       0.77      0.77      0.77       160\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction"
      ],
      "metadata": {
        "id": "sTw5Syz0SmYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Path to the dataset\n",
        "file_path = '/content/drive/Shareddrives/AI Text Detecting/mal_test_data_hum_ai.xlsx'  # Dataset path\n",
        "output_tsv_path = '/content/drive/Shareddrives/AI Text Detecting/malayalam_test_data_with_Ngenerated_labels.tsv'\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "try:\n",
        "    df = pd.read_excel(file_path)  # Load Excel file\n",
        "except UnicodeDecodeError:\n",
        "    df = pd.read_csv(file_path, encoding='latin1')  # Fallback for non-UTF-8 encoded CSV files\n",
        "\n",
        "# Debugging: Print column names\n",
        "print(\"Columns in dataset:\", df.columns)\n",
        "\n",
        "# Step 2: Process the dataset\n",
        "if 'DATA' in df.columns:\n",
        "    X = df['DATA']  # Use 'DATA' for features\n",
        "    # Assign \"Human\" and \"AI\" labels as an example (replace with actual label logic if available)\n",
        "    y = ['Human' if i % 2 == 0 else 'AI' for i in range(len(X))]  # Example: Alternate between Human and AI\n",
        "else:\n",
        "    raise ValueError(\"The dataset must contain a 'DATA' column for text data.\")\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train.astype(str))  # Convert to string\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test.astype(str))\n",
        "\n",
        "# Step 5: Train the Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "y_pred = nb_model.predict(X_test_tfidf)\n",
        "print(\"Naive Bayes with TF-IDF - Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 7: Predict labels for the entire dataset\n",
        "all_data_tfidf = tfidf_vectorizer.transform(X.astype(str))\n",
        "predicted_labels = nb_model.predict(all_data_tfidf)\n",
        "\n",
        "# Step 8: Add predicted labels to the dataset\n",
        "df['PREDICTED_LABEL'] = predicted_labels\n",
        "\n",
        "# Step 9: Save the results to a TSV file\n",
        "df.to_csv(output_tsv_path, sep='\\t', index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_tsv_path}\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBQMXUbz0K2J",
        "outputId": "51148a0c-5853-4786-defd-7ced53e0a1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in dataset: Index(['ID', 'DATA'], dtype='object')\n",
            "Naive Bayes with TF-IDF - Accuracy: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.43      0.75      0.55        16\n",
            "       Human       0.67      0.33      0.44        24\n",
            "\n",
            "    accuracy                           0.50        40\n",
            "   macro avg       0.55      0.54      0.49        40\n",
            "weighted avg       0.57      0.50      0.48        40\n",
            "\n",
            "Predictions saved to /content/drive/Shareddrives/AI Text Detecting/malayalam_test_data_with_Ngenerated_labels.tsv\n",
            "                ID                                               DATA  \\\n",
            "0  MAL_HUAI_TE_001  എല്ലാം നന്നായിട്ടുണ്ട് പക്ഷെ എനിക്ക് മീനിന്റെ ...   \n",
            "1  MAL_HUAI_TE_002  ബിലാൽ തട്ടുകടയിൽ നിന്നും പഴകിയ ഫുഡ് കുറെ പ്രാവ...   \n",
            "2  MAL_HUAI_TE_003  കോവയ്ക്ക ഉപ്പിലിട്ടത് എൻറെ ജീവിതത്തിൽ ഇതുവരെ ക...   \n",
            "3  MAL_HUAI_TE_004  കേരളത്തിൽ ഏറ്റവും നല്ല ഭക്ഷണം കിട്ടുന്നത് കോഴി...   \n",
            "4  MAL_HUAI_TE_005  എല്ലാം അടിപൊളി, പക്ഷെ സുരേന്ദ്രൻ തട്ടുകടയിൽ തി...   \n",
            "\n",
            "  PREDICTED_LABEL  \n",
            "0           Human  \n",
            "1           Human  \n",
            "2           Human  \n",
            "3              AI  \n",
            "4           Human  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert Model"
      ],
      "metadata": {
        "id": "jpd8zRN0fflB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets torch scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppZNTopNffEJ",
        "outputId": "048c4d06-a345-4934-de77-b981ea408fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fc1H0SrgI5q",
        "outputId": "f9bb7c81-498c-4215-c30d-aa82f800292f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrHmDBOughKc",
        "outputId": "84fc93b5-56b9-468d-e370-f9595720156c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx2kH8d4gkgx",
        "outputId": "0ae88f0f-d3f4-42b2-fbf7-c1eea1716c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-0GrM-ngnvf",
        "outputId": "b6d31ab8-4b46-40ad-f33e-4aba70e5b12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m877.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/drive/Shareddrives/AI Text Detecting/malayalam_dataset.csv\")  # Replace with your dataset's filename\n",
        "reviews = data[\"DATA\"]  # Replace 'DATA' with your column name for reviews\n",
        "labels = data[\"LABEL\"]  # Replace 'LABEL' with your column name for labels\n",
        "\n",
        "# Sample Malayalam stop words (expand this list as needed)\n",
        "malayalam_stopwords = [\n",
        "    'അത', 'ഇത്', 'അവന്', 'അവള്', 'എന്റെ', 'ഈ', 'എന്ന്', 'ഒരു', 'ഇല്ല', 'ഉള്ള',\n",
        "    'ഇല്', 'എനിക്കു', 'എന്റെ', 'അവൻ', 'അവളുടെ', 'നീ', 'ഇവിടെ'\n",
        "]\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess_text(text, stop_words):\n",
        "    # Tokenize and remove stop words\n",
        "    words = text.split()\n",
        "    cleaned_words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(cleaned_words)\n",
        "\n",
        "# Preprocess the reviews\n",
        "data[\"cleaned_review\"] = data[\"DATA\"].apply(lambda x: preprocess_text(x, malayalam_stopwords))\n",
        "\n",
        "# Encode labels to integers if not already done\n",
        "label_mapping = {label: idx for idx, label in enumerate(data[\"LABEL\"].unique())}\n",
        "data[\"encoded_label\"] = data[\"LABEL\"].map(label_mapping)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data[\"cleaned_review\"], data[\"encoded_label\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize data using a transformer tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Create custom dataset class\n",
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = ReviewsDataset(train_encodings, list(train_labels))\n",
        "test_dataset = ReviewsDataset(test_encodings, list(test_labels))\n",
        "\n",
        "# Load transformer model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", num_labels=len(label_mapping)\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    report_to = \"none\"\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./malayalam_review_model\")\n",
        "tokenizer.save_pretrained(\"./malayalam_review_model\")\n",
        "\n",
        "# Optional: Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", results)\n",
        "\n",
        "# Test the model with new data (optional)\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_class = torch.argmax(probs).item()\n",
        "    return {v: k for k, v in label_mapping.items()}[predicted_class]\n",
        "\n",
        "# Example prediction\n",
        "example_review = \"ഞാൻ കുറച്ച് കാലമായി മുച്ചട്ച്ചിൻ്റെ ഫേസ് വാഷ് ഉഭയോഗിക്കുന്നുണ്ട് നല്ല പ്രോഡക്റ്റ് ആണ്.\"\n",
        "print(\"Predicted label:\", predict(example_review))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577,
          "referenced_widgets": [
            "3a9ebe1ab8874afc8975b4e1d3d37778",
            "e2c6a8243ab5418e98e57004a1fa394f",
            "61eab914a5c44375ad22488f3b5f6ca4",
            "421b4547201c4a39af080da731817c88",
            "88a20fc0299745f1a19f73bb23d38729",
            "dcc0173ea9ea4af3bb94f1c3be8a15ce",
            "6b063bc0d78d45f68d82b455f3867463",
            "e61b35706a4140ddbc276bb3fec194c2",
            "c4afe7d1912f402d8db57163fbea9115",
            "88b00f9141e74c3e8119a3e0314cfd2a",
            "bc71094fde0d4a41af67a0e62ec22fe4",
            "11fa87899ae74d978ba63cb520e34179",
            "7047096c2a2e4de6bd952a6e03a2152e",
            "831b64562d014e3389833e5dbd108bf1",
            "86e309c5437744518aec97a11e45d7cf",
            "795b219b7bc041d7bbb32f368c6822f3",
            "305788c9261d40589cdf080d5ab7db1a",
            "90bae68c293e43a4b0e019e4e5d6781f",
            "018f0404108e4ebf8573de49c77f6f9f",
            "ab5902cfef7843c7809c00bce36be671",
            "c199e18efb2d4758a50103f606c3a8a8",
            "f4d435ba082b4e3ea80c282b5d0c1ae4",
            "7b347821fa2a4adcb9e9cafd36584542",
            "81e63b2ee3104a2798ed06bfaaf9e36e",
            "6fd26e1c76d64861b1cc56b6b558e589",
            "5e378c2724184eb28fadc4e9d0db39b9",
            "6579dfe512954eebb1cae8df81321ac0",
            "fd06ace1a2b14c45b5f5cd144f17d412",
            "f4e1c32f6b6740a0acf5f0889db5567a",
            "bf0f02d5bdc844e5b2e83dfbd212858d",
            "4bf8849a22a64810b2fe6af093371ed6",
            "cad6ee82c16b40e489f38a64dff7f5c3",
            "eb3ae7ac618d41e68c15b693d554fc6d",
            "c8525ab92e734a8e9679bd873d100b7c",
            "9b546f57062649ffbd25758f1bb969b0",
            "e3fbc8fad21a47c4bffbf7cdd1744d3a",
            "449de2e5bd9142788c177d44d9ea7e9d",
            "632cc56a4e084df5b4ad249c1a573b72",
            "8664ce7e8e51435587b915cff1c1b4b8",
            "238ab95a086e49a7bdd5d2561fdd27bf",
            "ca103bd950574fc9b4bafdcbcf6b1cb5",
            "a5754825a35a4585aec95732015a670d",
            "b2f5e198c69a4cfaa84006a6c38e2900",
            "a268308fde9a49a787ee645c2f3547df",
            "89e3d1e22d9e41b98856eece6ed7db20",
            "9c07bd21918a41fb96d781783e3e3ed5",
            "106e373f66824013b827b09de5c937b6",
            "24bd91783a6341f5bdf3ed97d99eeee3",
            "bb8346910f6342d382827c69a46276fe",
            "4d35f37b458a43759a6c4ecdf46fffc6",
            "33ac727e63434a54a75d8bdcf358f5a1",
            "63319dc15cd1412ebb0db8505ced66a5",
            "fce04d01bed04a2abb495830f1ad4c59",
            "d6f1edab71d14f3f98c0f14929e1069a",
            "debed1b5caa744bbb26857f25f02f76a"
          ]
        },
        "id": "EnHXftNcgrVY",
        "outputId": "d0dd5756-3351-4bf1-cad7-af6e463337a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a9ebe1ab8874afc8975b4e1d3d37778"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11fa87899ae74d978ba63cb520e34179"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b347821fa2a4adcb9e9cafd36584542"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8525ab92e734a8e9679bd873d100b7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89e3d1e22d9e41b98856eece6ed7db20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 52:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>0.198284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.140400</td>\n",
              "      <td>0.169896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.178555</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 01:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.178555428981781, 'eval_runtime': 70.591, 'eval_samples_per_second': 2.267, 'eval_steps_per_second': 0.142, 'epoch': 3.0}\n",
            "Predicted label: HUMAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions and true labels\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "true_labels = test_labels.tolist()\n",
        "class_report = classification_report(\n",
        "    true_labels,\n",
        "    predicted_labels,\n",
        "    target_names=[label for label in label_mapping.keys()],\n",
        "    zero_division=0\n",
        ")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "0nAjjajhhUY7",
        "outputId": "22612901-9591-44a4-a5d2-0713c05d96d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HUMAN       1.00      0.91      0.95        80\n",
            "          AI       0.92      1.00      0.96        80\n",
            "\n",
            "    accuracy                           0.96       160\n",
            "   macro avg       0.96      0.96      0.96       160\n",
            "weighted avg       0.96      0.96      0.96       160\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction"
      ],
      "metadata": {
        "id": "jOhBfuqLSz_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Paths to files and model\n",
        "test_data_path = \"/content/drive/Shareddrives/AI Text Detecting/mal_test_data_hum_ai.xlsx\"  # Replace with your test data path\n",
        "model_path = \"./malayalam_review_model\"  # Path to your trained Malayalam model\n",
        "output_tsv_path = \"/content/drive/Shareddrives/AI Text Detecting/malayalam_test_data_with_generated_labels.tsv\"  # Path for the output TSV file\n",
        "\n",
        "# Load test data from .xlsx file\n",
        "test_data = pd.read_excel(test_data_path)  # Adjust if your file has multiple sheets\n",
        "test_texts = test_data[\"DATA\"]\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Function to make predictions\n",
        "def predict(texts):\n",
        "    predictions = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(probs).item()\n",
        "        predictions.append(predicted_class)\n",
        "    return predictions\n",
        "\n",
        "# Generate predictions\n",
        "predicted_labels = predict(test_texts)\n",
        "\n",
        "# Map predictions to AI or Human (modify as per your trained labels)\n",
        "label_mapping = {0: \"Human\", 1: \"AI\"}  # Adjust based on your trained labels\n",
        "\n",
        "# Create the LABEL column with predictions\n",
        "test_data[\"LABEL\"] = [label_mapping[label] for label in predicted_labels]\n",
        "\n",
        "# Save the results to a TSV file\n",
        "test_data.to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_tsv_path}\")\n",
        "\n",
        "# Print out the first few rows of the resulting DataFrame\n",
        "print(test_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVOwVJztk8cq",
        "outputId": "8db530a1-7048-4ed6-9834-29b175b77993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/drive/Shareddrives/AI Text Detecting/malayalam_test_data_with_generated_labels.tsv\n",
            "                ID                                               DATA  LABEL\n",
            "0  MAL_HUAI_TE_001  എല്ലാം നന്നായിട്ടുണ്ട് പക്ഷെ എനിക്ക് മീനിന്റെ ...  Human\n",
            "1  MAL_HUAI_TE_002  ബിലാൽ തട്ടുകടയിൽ നിന്നും പഴകിയ ഫുഡ് കുറെ പ്രാവ...  Human\n",
            "2  MAL_HUAI_TE_003  കോവയ്ക്ക ഉപ്പിലിട്ടത് എൻറെ ജീവിതത്തിൽ ഇതുവരെ ക...  Human\n",
            "3  MAL_HUAI_TE_004  കേരളത്തിൽ ഏറ്റവും നല്ല ഭക്ഷണം കിട്ടുന്നത് കോഴി...  Human\n",
            "4  MAL_HUAI_TE_005  എല്ലാം അടിപൊളി, പക്ഷെ സുരേന്ദ്രൻ തട്ടുകടയിൽ തി...  Human\n"
          ]
        }
      ]
    }
  ]
}